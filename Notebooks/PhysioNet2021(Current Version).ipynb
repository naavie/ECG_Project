{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhysioNet 2021 Challenge\n",
    "\n",
    "The training data contains twelve-lead ECGs. The validation and test data contains twelve-lead, six-lead, four-lead, three-lead, and two-lead ECGs:\n",
    "\n",
    "1. Twelve leads: I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6\n",
    "2. Six leads: I, II, III, aVR, aVL, aVF\n",
    "3. Four leads: I, II, III, V2\n",
    "4. Three leads: I, II, V2\n",
    "5. Two leads: I, II\n",
    "\n",
    "Each ECG recording has one or more labels that describe cardiac abnormalities (and/or a normal sinus rhythm).\n",
    "\n",
    "The Challenge data include annotated twelve-lead ECG recordings from six sources in four countries across three continents. These databases include over 100,000 twelve-lead ECG recordings with over 88,000 ECGs shared publicly as training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a header file A0001.hea may have the following contents:\n",
    "\n",
    "```\n",
    "    A0001 12 500 7500\n",
    "    A0001.mat 16+24 1000/mV 16 0 28 -1716 0 I\n",
    "    A0001.mat 16+24 1000/mV 16 0 7 2029 0 II\n",
    "    A0001.mat 16+24 1000/mV 16 0 -21 3745 0 III\n",
    "    A0001.mat 16+24 1000/mV 16 0 -17 3680 0 aVR\n",
    "    A0001.mat 16+24 1000/mV 16 0 24 -2664 0 aVL\n",
    "    A0001.mat 16+24 1000/mV 16 0 -7 -1499 0 aVF\n",
    "    A0001.mat 16+24 1000/mV 16 0 -290 390 0 V1\n",
    "    A0001.mat 16+24 1000/mV 16 0 -204 157 0 V2\n",
    "    A0001.mat 16+24 1000/mV 16 0 -96 -2555 0 V3\n",
    "    A0001.mat 16+24 1000/mV 16 0 -112 49 0 V4\n",
    "    A0001.mat 16+24 1000/mV 16 0 -596 -321 0 V5\n",
    "    A0001.mat 16+24 1000/mV 16 0 -16 -3112 0 V6\n",
    "    #Age: 74\n",
    "    #Sex: Male\n",
    "    #Dx: 426783006\n",
    "    #Rx: Unknown\n",
    "    #Hx: Unknown\n",
    "    #Sx: Unknown\n",
    "```\n",
    "\n",
    "From the first line of the file:\n",
    "- We see that the recording number is A0001, and the recording file is A0001.mat. \n",
    "- The recording has 12 leads, each recorded at a 500 Hz sampling frequency, and contains 7500 samples. \n",
    "- From the next 12 lines of the file (one for each lead), we see that each signal:\n",
    "    - Was written at 16 bits with an offset of 24 bits\n",
    "    - The floating point number (analog-to-digital converter (ADC) units per physical unit) is 1000/mV \n",
    "    - The resolution of the analog-to-digital converter (ADC) used to digitize the signal is 16 bits, and the baseline value corresponding to 0 physical units is 0. \n",
    "    - The first value of the signal (-1716, etc.), the checksum (0, etc.), and the lead name (I, etc.) are the last three entries of each of these lines. \n",
    "- From the final 6 lines, we see that the patient is:\n",
    "    - A 74-year-old male \n",
    "    - With a diagnosis (Dx) of 426783006, which is the **SNOMED-CT code** for sinus rhythm. \n",
    "    - The medical prescription (Rx), history (Hx), and symptom or surgery (Sx) are unknown. \n",
    "\n",
    "- Please visit WFDB header format for more information on the header file and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "\n",
    "# Matlab/WFDB files\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/navme/Desktop/ECG_Project/PyFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataset import PhysioNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PhysioNet_PATH = f'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'\n",
    "PhysioNet_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysioNetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, train=False):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset_path = [path for path in self.dataset_path if \"index.html\" not in path]\n",
    "        self.train = train\n",
    "        self.file_list = os.listdir(dataset_path)\n",
    "        self._hea_files = []\n",
    "        self._mat_files = []\n",
    "        self._indices_files = []\n",
    "        self._hea_files_path = []\n",
    "        self._mat_files_path = []\n",
    "\n",
    "        self.file_PATHS = []  # Directory to main database folders\n",
    "        self.data_files = []  # Directory to data files\n",
    "\n",
    "        # Validation Case: PTB Databases only\n",
    "        if self.train == False:\n",
    "            validation_datasets = ['ptb', 'ptb-xl']\n",
    "            for file in os.listdir(dataset_path):\n",
    "                if file in validation_datasets:\n",
    "                    file_path = os.path.join(dataset_path, file)\n",
    "                    file_path = file_path.replace('\\\\', '/')\n",
    "                    self.file_PATHS.append(file_path)\n",
    "\n",
    "        # Training Case: All Databases excluding PTB\n",
    "        else:\n",
    "            validation_datasets = ['ptb', 'ptb-xl']\n",
    "            for file in os.listdir(dataset_path):\n",
    "                if file not in validation_datasets:\n",
    "                    file_path = os.path.join(dataset_path, file)\n",
    "                    file_path = file_path.replace('\\\\', '/')\n",
    "                    self.file_PATHS.append(file_path)\n",
    "\n",
    "        for path in self.file_PATHS:\n",
    "            if os.path.isdir(path):\n",
    "                for sub_folder in os.listdir(path):\n",
    "                    sub_folder_path = os.path.join(path, sub_folder)\n",
    "                    sub_folder_path = sub_folder_path.replace('\\\\', '/')\n",
    "                    \n",
    "                    # Ignore index.html files\n",
    "                    if sub_folder_path.endswith('index.html'):\n",
    "                        self._indices_files.append(sub_folder_path)\n",
    "                    else:\n",
    "                        if os.path.isdir(sub_folder_path):\n",
    "                            for file in os.listdir(sub_folder_path):\n",
    "                                # Get all .hea files\n",
    "                                if file.endswith('.hea'):\n",
    "                                    file_path = os.path.join(sub_folder_path, file)\n",
    "                                    file_path = file_path.replace('\\\\', '/')\n",
    "                                    self._hea_files.append(file_path)\n",
    "                                    self._hea_files_path.append(file_path)\n",
    "                                # Get all .mat files\n",
    "                                elif file.endswith('.mat'):\n",
    "                                    file_path = os.path.join(sub_folder_path, file)\n",
    "                                    file_path = file_path.replace('\\\\', '/')\n",
    "                                    self._mat_files.append(file_path)\n",
    "                                    self._mat_files_path.append(file_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 1. Get .hea file\n",
    "        hea_file_path = self._hea_files[index]\n",
    "        with open(hea_file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # Parse header information\n",
    "        # Initialize header information\n",
    "        header_info = {\n",
    "            'recording_number': lines[0].split()[0],\n",
    "            'recording_file': lines[0].split()[0] + '.mat',\n",
    "            'num_leads': int(lines[0].split()[1]),\n",
    "            'sampling_frequency': int(lines[0].split()[2]),\n",
    "            'num_samples': int(lines[0].split()[3]),\n",
    "            'leads_info': [],\n",
    "            'age': None,\n",
    "            'sex': None,\n",
    "            'dx': None,\n",
    "            'rx': None,\n",
    "            'hx': None,\n",
    "            'sx': None,\n",
    "        }\n",
    "\n",
    "        # Parse header information\n",
    "        for line in lines:\n",
    "            if line.startswith('# Age:'):\n",
    "                age_str = line.split(':')[1].strip()\n",
    "                header_info['age'] = int(age_str) if age_str != 'NaN' else None\n",
    "            elif line.startswith('# Sex:'):\n",
    "                header_info['sex'] = line.split(':')[1].strip()\n",
    "            elif line.startswith('# Dx:'):\n",
    "                header_info['dx'] = line.split(':')[1].strip().split(',')\n",
    "            elif line.startswith('# Rx:'):\n",
    "                header_info['rx'] = line.split(':')[1].strip()\n",
    "            elif line.startswith('# Hx:'):\n",
    "                header_info['hx'] = line.split(':')[1].strip()\n",
    "            elif line.startswith('# Sx:'):\n",
    "                header_info['sx'] = line.split(':')[1].strip()\n",
    "\n",
    "        for line in lines[1:header_info['num_leads']+1]:\n",
    "            adc_gain = line.split()[2].split('/')[0]\n",
    "            adc_gain = float(adc_gain.replace('(0)', ''))  # Remove '(0)' and convert to float\n",
    "            lead_info = {\n",
    "                'file': line.split()[0],\n",
    "                'adc_gain': adc_gain,\n",
    "                'units': line.split()[2].split('/')[1],\n",
    "                'adc_resolution': int(line.split()[3]),\n",
    "                'adc_zero': int(line.split()[4]),\n",
    "                'initial_value': int(line.split()[5]),\n",
    "                'checksum': int(line.split()[6]),\n",
    "                'lead_name': line.split()[7],\n",
    "            }\n",
    "            header_info['leads_info'].append(lead_info)\n",
    "\n",
    "        # 2. Get .mat file\n",
    "        twelve_lead_ecg = None\n",
    "        if index < len(self._mat_files):\n",
    "            mat_file_path = self._mat_files[index]\n",
    "            twelve_lead_ecg = sio.loadmat(mat_file_path)\n",
    "        else:\n",
    "            print(f\"MAT file for index {index} does not exist.\")\n",
    "        \n",
    "        return header_info, twelve_lead_ecg\n",
    "\n",
    "    def plot_record(self, index):\n",
    "        mat_file_path = self._mat_files[index]\n",
    "        data = sio.loadmat(mat_file_path)\n",
    "        fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(12, 8))\n",
    "\n",
    "        for i, ax in enumerate(axs.flat):\n",
    "            ax.plot(data['val'][i])\n",
    "            ax.set_xlabel('Sample')\n",
    "            ax.set_ylabel('Amplitude')\n",
    "            ax.set_title(f'Lead {i+1}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._hea_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_set & val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PhysioNetDataset(PhysioNet_PATH, train = True)\n",
    "val_set = PhysioNetDataset(PhysioNet_PATH, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 65900/65900 [01:08<00:00, 960.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAT file for index 65892 does not exist.\n",
      "MAT file for index 65893 does not exist.\n",
      "MAT file for index 65894 does not exist.\n",
      "MAT file for index 65895 does not exist.\n",
      "MAT file for index 65896 does not exist.\n",
      "MAT file for index 65897 does not exist.\n",
      "MAT file for index 65898 does not exist.\n",
      "MAT file for index 65899 does not exist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 65900 records.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the records\n",
    "records = []\n",
    "\n",
    "# Iterate over all records\n",
    "for i in tqdm(range(len(train_set)), desc=\"Processing records\"):\n",
    "    record, _ = train_set[i]  # Get the record (ignore the ECG data for now)\n",
    "    \n",
    "    # Flatten the 'leads_info' list into separate columns for each lead\n",
    "    for j, lead_info in enumerate(record['leads_info']):\n",
    "        for key, value in lead_info.items():\n",
    "            record[f'lead_{j}_{key}'] = value\n",
    "    del record['leads_info']  # We don't need the 'leads_info' list anymore\n",
    "\n",
    "    # Append the record to the list\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the list of records into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('train_set_records.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 22352/22352 [17:27<00:00, 21.34it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAT file for index 22349 does not exist.\n",
      "MAT file for index 22350 does not exist.\n",
      "MAT file for index 22351 does not exist.\n",
      "Processed 22352 records.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the records\n",
    "records = []\n",
    "\n",
    "# Iterate over all records\n",
    "for i in tqdm(range(len(val_set)), desc=\"Processing records\"):\n",
    "    record, _ = val_set[i]  # Get the record (ignore the ECG data for now)\n",
    "    \n",
    "    # Flatten the 'leads_info' list into separate columns for each lead\n",
    "    for j, lead_info in enumerate(record['leads_info']):\n",
    "        for key, value in lead_info.items():\n",
    "            record[f'lead_{j}_{key}'] = value\n",
    "    del record['leads_info']  # We don't need the 'leads_info' list anymore\n",
    "\n",
    "    # Append the record to the list\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the list of records into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('val_set_records.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df = pd.read_csv('train_set_records.csv')\n",
    "val_set_df = pd.read_csv('val_set_records.csv')\n",
    "# train_set_records['dx'] = train_set_records['dx'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_records.iloc[150]['dx']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
