{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Thesis Topic: “Zero-shot classification of ECG signals using CLIP-like model”. \n",
    "\n",
    "**For example: Train on PBT-XL:** \n",
    "\n",
    "- Text Encoder: ClinicalBERT (trained on diagnoses of ECG signal to obtain corresponding embeddings)\n",
    "- Image Encoder: 1D-CNN (used to encode ECG signal to obtain signal embeddings)\n",
    "\n",
    "- Experiment A): Baseline: We can take only the name of the class. For example, take “Myocardial Infarction” as a text. We should exclude some classes from training and after training is completed, the CLIP-like model can be tested on these excluded classes. \n",
    "    - Next, we get embeddings of text from ClinicalBERT and train the ECG encoder with contrastive loss.\n",
    "\n",
    "- Experiment B): Same as Experiment A but instead of testing on the same dataset/classes, we would test on other datasets containing different classes.\n",
    "\n",
    "**Evaluation metrics:** \n",
    "- Main: AUC-ROC, average_precison_score, \n",
    "- Optional: Specificity, Sensitivity, F1-score \n",
    "\n",
    "**Outcome:** \n",
    "- It’s possible to train CLIP-like models with freezed (or unchanged/not fine tuned for downstream tasks) text encoder\n",
    "- Training ECG encoders that are viable for representing different domains (within ECG modality) and previously unseen classes. \n",
    "- Training a CLIP-like model on ECGs has little novelty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class ECG Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we preprocess the ECG data from the PhysioNet 2021 challenge dataset. This data will be loaded using the ```PhysioNetDataset``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import ast\n",
    "import scipy.io as sio\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/navme/Desktop/ECG_Project/PyFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to dir/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training\n",
    "PhysioNet_PATH = f'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'\n",
    "PhysioNet_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ```PhysioNet_PATH```, we can create separate datasets for training, testing & validation.\n",
    "\n",
    "# Data Preprocessing \n",
    "\n",
    "- train_set (train & validation data)\n",
    "- test_set (test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65900, 22352)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = PhysioNetDataset(PhysioNet_PATH, train=True)\n",
    "test_set = PhysioNetDataset(PhysioNet_PATH, train=False)\n",
    "\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```train_set``` can be split into ```current_train``` and ```current_val```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for the random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Get the length of the train_set\n",
    "length = len(train_set)\n",
    "\n",
    "# Calculate the lengths of the splits\n",
    "train_length = int(0.85 * length)\n",
    "val_length = length - train_length\n",
    "\n",
    "# Split the dataset\n",
    "current_train, current_val = random_split(train_set, [train_length, val_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to extract the header data for ```current_train```, ```current_val```, and ```test_set``` and save the data to a csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 56015/56015 [14:35<00:00, 63.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 56015 records.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the records\n",
    "records = []\n",
    "\n",
    "# Iterate over all records\n",
    "for i in tqdm(range(len(current_train)), desc=\"Processing records\"):\n",
    "    record, _ = train_set[i]  # Get the record (ignore the ECG data for now)\n",
    "    \n",
    "    # Flatten the 'leads_info' list into separate columns for each lead\n",
    "    for j, lead_info in enumerate(record['leads_info']):\n",
    "        for key, value in lead_info.items():\n",
    "            record[f'lead_{j}_{key}'] = value\n",
    "    del record['leads_info']  # We don't need the 'leads_info' list anymore\n",
    "\n",
    "    # Append the record to the list\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the list of records into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('train_set_records.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 9885/9885 [00:15<00:00, 634.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9885 records.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the records\n",
    "records = []\n",
    "\n",
    "# Iterate over all records\n",
    "for i in tqdm(range(len(current_val)), desc=\"Processing records\"):\n",
    "    record, _ = train_set[i]  # Get the record (ignore the ECG data for now)\n",
    "    \n",
    "    # Flatten the 'leads_info' list into separate columns for each lead\n",
    "    for j, lead_info in enumerate(record['leads_info']):\n",
    "        for key, value in lead_info.items():\n",
    "            record[f'lead_{j}_{key}'] = value\n",
    "    del record['leads_info']  # We don't need the 'leads_info' list anymore\n",
    "\n",
    "    # Append the record to the list\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the list of records into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('val_set_records.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 22352/22352 [00:43<00:00, 510.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22352 records.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the records\n",
    "records = []\n",
    "\n",
    "# Iterate over all records\n",
    "for i in tqdm(range(len(test_set)), desc=\"Processing records\"):\n",
    "    record, _ = train_set[i]  # Get the record (ignore the ECG data for now)\n",
    "    \n",
    "    # Flatten the 'leads_info' list into separate columns for each lead\n",
    "    for j, lead_info in enumerate(record['leads_info']):\n",
    "        for key, value in lead_info.items():\n",
    "            record[f'lead_{j}_{key}'] = value\n",
    "    del record['leads_info']  # We don't need the 'leads_info' list anymore\n",
    "\n",
    "    # Append the record to the list\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the list of records into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('test_set_records.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the header data has been extracted and saved to csv files, we can map the corresponding SNOWMED-CT code to the csv files too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the SNOWMED-CT mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dx</th>\n",
       "      <th>SNOMEDCTCode</th>\n",
       "      <th>Abbreviation</th>\n",
       "      <th>CPSC</th>\n",
       "      <th>CPSC_Extra</th>\n",
       "      <th>StPetersburg</th>\n",
       "      <th>PTB</th>\n",
       "      <th>PTB_XL</th>\n",
       "      <th>Georgia</th>\n",
       "      <th>Chapman_Shaoxing</th>\n",
       "      <th>Ningbo</th>\n",
       "      <th>Total</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atrial fibrillation</td>\n",
       "      <td>164889003</td>\n",
       "      <td>AF</td>\n",
       "      <td>1221</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1514</td>\n",
       "      <td>570</td>\n",
       "      <td>1780</td>\n",
       "      <td>0</td>\n",
       "      <td>5255</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atrial flutter</td>\n",
       "      <td>164890007</td>\n",
       "      <td>AFL</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>186</td>\n",
       "      <td>445</td>\n",
       "      <td>7615</td>\n",
       "      <td>8374</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Dx  SNOMEDCTCode Abbreviation  CPSC  CPSC_Extra  \\\n",
       "0  atrial fibrillation     164889003           AF  1221         153   \n",
       "1       atrial flutter     164890007          AFL     0          54   \n",
       "\n",
       "   StPetersburg  PTB  PTB_XL  Georgia  Chapman_Shaoxing  Ningbo  Total Notes  \n",
       "0             2   15    1514      570              1780       0   5255   NaN  \n",
       "1             0    1      73      186               445    7615   8374   NaN  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smowmed_mappings_path = r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\SNOWMED-CT Codes\\combined_mappings.csv'\n",
    "smowmed_mappings_path = convert_to_forward_slashes(smowmed_mappings_path)\n",
    "\n",
    "# Load the SNOMED-CT mappings\n",
    "smowmed_mappings = pd.read_csv(smowmed_mappings_path)\n",
    "smowmed_mappings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'Dx' and 'SNOMEDCTCode' columns\n",
    "codes = smowmed_mappings[['Dx', 'SNOMEDCTCode']]\n",
    "\n",
    "# Set 'SNOWMEDCTCode' as the index\n",
    "codes.set_index('SNOMEDCTCode', inplace=True)\n",
    "\n",
    "# Convert the DataFrame into a dictionary\n",
    "codes_dict = codes['Dx'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the csv files and map the corresponding codes from ```codes_dict``` to the csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_path = convert_to_forward_slashes(r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\train_set_records.csv')\n",
    "val_set_path = convert_to_forward_slashes(r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\val_set_records.csv')\n",
    "test_set_path = convert_to_forward_slashes(r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\test_set_records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
