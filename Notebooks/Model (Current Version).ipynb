{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Thesis Topic: “Zero-shot classification of ECG signals using CLIP-like model”. \n",
    "\n",
    "**For example: Train on PBT-XL:** \n",
    "\n",
    "- Text Encoder: ClinicalBERT (trained on diagnoses of ECG signal to obtain corresponding embeddings)\n",
    "- Image Encoder: 1D-CNN (used to encode ECG signal to obtain signal embeddings)\n",
    "\n",
    "- Experiment A): Baseline: We can take only the name of the class. For example, take “Myocardial Infarction” as a text. We should exclude some classes from training and after training is completed, the CLIP-like model can be tested on these excluded classes. \n",
    "    - Next, we get embeddings of text from ClinicalBERT and train the ECG encoder with contrastive loss.\n",
    "\n",
    "- Experiment B): Same as Experiment A but instead of testing on the same dataset/classes, we would test on other datasets containing different classes.\n",
    "\n",
    "**Evaluation metrics:** \n",
    "- Main: AUC-ROC, average_precison_score, \n",
    "- Optional: Specificity, Sensitivity, F1-score \n",
    "\n",
    "**Outcome:** \n",
    "- It’s possible to train CLIP-like models with freezed (or unchanged/not fine tuned for downstream tasks) text encoder\n",
    "- Training ECG encoders that are viable for representing different domains (within ECG modality) and previously unseen classes. \n",
    "- Training a CLIP-like model on ECGs has little novelty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class ECG Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we preprocess the ECG data from the PhysioNet 2021 challenge dataset. This data will be loaded using the ```PhysioNetDataset``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import ast\n",
    "import scipy.io as sio\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/navme/Desktop/ECG_Project/PyFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to dir/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training\n",
    "PhysioNet_PATH = f'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'\n",
    "PhysioNet_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ```PhysioNet_PATH```, we can create separate datasets for training, testing & validation.\n",
    "\n",
    "# Data Preprocessing \n",
    "\n",
    "- train_set (train & validation data)\n",
    "- test_set (test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65900, 22352)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = PhysioNetDataset(PhysioNet_PATH, train=True)\n",
    "test_set = PhysioNetDataset(PhysioNet_PATH, train=False)\n",
    "\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```train_set``` can be split into ```current_train``` and ```current_val```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for the random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Get the length of the train_set\n",
    "length = len(train_set)\n",
    "\n",
    "# Calculate the lengths of the splits\n",
    "train_length = int(0.85 * length)\n",
    "val_length = length - train_length\n",
    "\n",
    "# Split the dataset\n",
    "current_train, current_val = random_split(train_set, [train_length, val_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to extract the header data for ```current_train```, ```current_val```, and ```test_set``` and save the data to a csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 56015/56015 [14:35<00:00, 63.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 56015 records.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the records\n",
    "records = []\n",
    "\n",
    "# Iterate over all records\n",
    "for i in tqdm(range(len(current_train)), desc=\"Processing records\"):\n",
    "    record, _ = train_set[i]  # Get the record (ignore the ECG data for now)\n",
    "    \n",
    "    # Flatten the 'leads_info' list into separate columns for each lead\n",
    "    for j, lead_info in enumerate(record['leads_info']):\n",
    "        for key, value in lead_info.items():\n",
    "            record[f'lead_{j}_{key}'] = value\n",
    "    del record['leads_info']  # We don't need the 'leads_info' list anymore\n",
    "\n",
    "    # Append the record to the list\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the list of records into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('train_set_records.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 9885/9885 [00:15<00:00, 634.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 9885 records.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the records\n",
    "records = []\n",
    "\n",
    "# Iterate over all records\n",
    "for i in tqdm(range(len(current_val)), desc=\"Processing records\"):\n",
    "    record, _ = train_set[i]  # Get the record (ignore the ECG data for now)\n",
    "    \n",
    "    # Flatten the 'leads_info' list into separate columns for each lead\n",
    "    for j, lead_info in enumerate(record['leads_info']):\n",
    "        for key, value in lead_info.items():\n",
    "            record[f'lead_{j}_{key}'] = value\n",
    "    del record['leads_info']  # We don't need the 'leads_info' list anymore\n",
    "\n",
    "    # Append the record to the list\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the list of records into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('val_set_records.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 22352/22352 [00:43<00:00, 510.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 22352 records.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the records\n",
    "records = []\n",
    "\n",
    "# Iterate over all records\n",
    "for i in tqdm(range(len(test_set)), desc=\"Processing records\"):\n",
    "    record, _ = train_set[i]  # Get the record (ignore the ECG data for now)\n",
    "    \n",
    "    # Flatten the 'leads_info' list into separate columns for each lead\n",
    "    for j, lead_info in enumerate(record['leads_info']):\n",
    "        for key, value in lead_info.items():\n",
    "            record[f'lead_{j}_{key}'] = value\n",
    "    del record['leads_info']  # We don't need the 'leads_info' list anymore\n",
    "\n",
    "    # Append the record to the list\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the list of records into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('test_set_records.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the header data has been extracted and saved to csv files, we can map the corresponding SNOWMED-CT code to the csv files too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the SNOWMED-CT mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dx</th>\n",
       "      <th>SNOMEDCTCode</th>\n",
       "      <th>Abbreviation</th>\n",
       "      <th>CPSC</th>\n",
       "      <th>CPSC_Extra</th>\n",
       "      <th>StPetersburg</th>\n",
       "      <th>PTB</th>\n",
       "      <th>PTB_XL</th>\n",
       "      <th>Georgia</th>\n",
       "      <th>Chapman_Shaoxing</th>\n",
       "      <th>Ningbo</th>\n",
       "      <th>Total</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atrial fibrillation</td>\n",
       "      <td>164889003</td>\n",
       "      <td>AF</td>\n",
       "      <td>1221</td>\n",
       "      <td>153</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>1514</td>\n",
       "      <td>570</td>\n",
       "      <td>1780</td>\n",
       "      <td>0</td>\n",
       "      <td>5255</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atrial flutter</td>\n",
       "      <td>164890007</td>\n",
       "      <td>AFL</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>186</td>\n",
       "      <td>445</td>\n",
       "      <td>7615</td>\n",
       "      <td>8374</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Dx  SNOMEDCTCode Abbreviation  CPSC  CPSC_Extra  \\\n",
       "0  atrial fibrillation     164889003           AF  1221         153   \n",
       "1       atrial flutter     164890007          AFL     0          54   \n",
       "\n",
       "   StPetersburg  PTB  PTB_XL  Georgia  Chapman_Shaoxing  Ningbo  Total Notes  \n",
       "0             2   15    1514      570              1780       0   5255   NaN  \n",
       "1             0    1      73      186               445    7615   8374   NaN  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smowmed_mappings_path = r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\SNOWMED-CT Codes\\combined_mappings.csv'\n",
    "smowmed_mappings_path = convert_to_forward_slashes(smowmed_mappings_path)\n",
    "\n",
    "# Load the SNOMED-CT mappings\n",
    "smowmed_mappings = pd.read_csv(smowmed_mappings_path)\n",
    "smowmed_mappings.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'Dx' and 'SNOMEDCTCode' columns\n",
    "codes = smowmed_mappings[['Dx', 'SNOMEDCTCode']]\n",
    "\n",
    "# Set 'SNOWMEDCTCode' as the index\n",
    "codes.set_index('SNOMEDCTCode', inplace=True)\n",
    "\n",
    "# Convert the DataFrame into a dictionary\n",
    "codes_dict = codes['Dx'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the csv files and map the corresponding codes from ```codes_dict``` to the csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_path = convert_to_forward_slashes(r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\train_set_records.csv')\n",
    "val_set_path = convert_to_forward_slashes(r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\val_set_records.csv')\n",
    "test_set_path = convert_to_forward_slashes(r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\test_set_records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df = load_and_process(train_set_path)\n",
    "val_set_df = load_and_process(val_set_path)\n",
    "test_set_df = load_and_process(test_set_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the ```map_codes_to_dx()``` function, let's map the SNOWMED-CT codes for each ECG signal ```dx```. The new column containing the diagnosis name will be ```dx_modality``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_codes_to_dx(codes):\n",
    "    return [codes_dict.get(int(code), code) for code in codes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df['dx_modality'] = train_set_df['dx'].apply(map_codes_to_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_df['dx_modality'] = val_set_df['dx'].apply(map_codes_to_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df['dx_modality'] = test_set_df['dx'].apply(map_codes_to_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atrial fibrillation', 'right bundle branch block', 't wave abnormal']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_df['dx_modality'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's save the updated csv files to new csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df.to_csv('processed_train_set_records.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_df.to_csv('processed_val_set_records.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df.to_csv('processed_test_set_records.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that data preprocessing is completed, we can proceed and build out DL pipeline.\n",
    "\n",
    "# DL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_set_path = convert_to_forward_slashes(r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\processed_train_set_records.csv')\n",
    "processed_val_set_path = convert_to_forward_slashes(r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\processed_val_set_records.csv')\n",
    "processed_test_set_path = convert_to_forward_slashes(r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\processed_test_set_records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df = pd.read_csv(processed_train_set_path)\n",
    "processed_val_df = pd.read_csv(processed_val_set_path)\n",
    "processed_test_df = pd.read_csv(processed_test_set_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the model's pipeline is to create: \n",
    "\n",
    "## TextEncoder()\n",
    "\n",
    "Create a class, ```TextEncoder()``` that is used to convert the description of the (dx_modality) diagnosis class into embeddings using the ClinicalBERT model.\n",
    "\n",
    "- Input should be a concatenated using comma or blank space string of diagnoses/dx_modality per ECG signal.\n",
    "- Use processed CSV files (dx_modality vs dx_modality, age, etc together)\n",
    "- Frozen weights (since it's already pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "    def encode(self, text_list):\n",
    "        # Check if text_list is a string representation of a list\n",
    "        if isinstance(text_list, str):\n",
    "            text_list = ast.literal_eval(text_list)\n",
    "        # Convert list of strings to a single string\n",
    "        text = ', '.join(text_list)\n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        # Get embeddings from ClinicalBERT model\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state\n",
    "        # Average the embeddings to get single vector per each input\n",
    "        embeddings = torch.mean(embeddings, dim=1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if isinstance(processed_train_df['dx_modality'][4], str):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example of TextEncoder\n",
    "encoder = TextEncoder()\n",
    "embeddings = encoder.encode(processed_train_df['dx_modality'][0])\n",
    "\n",
    "# Check size of the embeddings\n",
    "print(embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps in the pipeline are to create: \n",
    "\n",
    "## 1. 1D-CNN Model\n",
    "\n",
    "This 1D-CNN will be used as the input for the ```ECGEncoder()```\n",
    "\n",
    "## 2. ECGEncoder() \n",
    "\n",
    "- Input is ECG signal, output will be embeddings of ECG signal\n",
    "- This is going to be model in model.py \n",
    "- Model weights are updated iteratively\n",
    "- optimizer = torch.optim.Adam(clip_model.ECGEncoder.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 1D-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OneDimCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(OneDimCNN, self).__init__()\n",
    "\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 3\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 4\n",
    "        self.conv4 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 5\n",
    "        self.conv5 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.pool5 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 6\n",
    "        self.conv6 = nn.Conv1d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm1d(1024)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.pool6 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 7\n",
    "        self.conv7 = nn.Conv1d(in_channels=1024, out_channels=2048, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn7 = nn.BatchNorm1d(2048)\n",
    "        self.relu7 = nn.ReLU()\n",
    "        self.pool7 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 8\n",
    "        self.conv8 = nn.Conv1d(in_channels=2048, out_channels=4096, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn8 = nn.BatchNorm1d(4096)\n",
    "        self.relu8 = nn.ReLU()\n",
    "        self.pool8 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 9\n",
    "        self.conv9 = nn.Conv1d(in_channels=4096, out_channels=8192, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn9 = nn.BatchNorm1d(8192)\n",
    "        self.relu9 = nn.ReLU()\n",
    "        self.pool9 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 10\n",
    "        self.conv10 = nn.Conv1d(in_channels=8192, out_channels=16384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn10 = nn.BatchNorm1d(16384)\n",
    "        self.relu10 = nn.ReLU()\n",
    "        self.pool10 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 11\n",
    "        self.conv11 = nn.Conv1d(in_channels=16384, out_channels=32768, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn11 = nn.BatchNorm1d(32768)\n",
    "        self.relu11 = nn.ReLU()\n",
    "        self.pool11 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 12\n",
    "        self.conv12 = nn.Conv1d(in_channels=32768, out_channels=65536, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn12 = nn.BatchNorm1d(65536)\n",
    "        self.relu12 = nn.ReLU()\n",
    "        self.pool12 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        self.fc = nn.Linear(65536, num_classes)\n",
    "        self.relu13 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        self.fc2 = nn.Linear(num_classes, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.pool1(out)\n",
    "\n",
    "        # Layer 2\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.pool2(out)\n",
    "\n",
    "        # Layer 3\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.pool3(out)\n",
    "\n",
    "        # Layer 4\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.pool4(out)\n",
    "\n",
    "        # Layer 5\n",
    "        out = self.conv5(out)\n",
    "        out = self.bn5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.pool5(out)\n",
    "\n",
    "        # Layer 6\n",
    "        out = self.conv6(out)\n",
    "        out = self.bn6(out)\n",
    "        out = self.relu6(out)\n",
    "        out = self.pool6(out)\n",
    "\n",
    "        # Layer 7\n",
    "        out = self.conv7(out)\n",
    "        out = self.bn7(out)\n",
    "        out = self.relu7(out)\n",
    "        out = self.pool7(out)\n",
    "\n",
    "        # Layer 8\n",
    "        out = self.conv8(out)\n",
    "        out = self.bn8(out)\n",
    "        out = self.relu8(out)\n",
    "        out = self.pool8(out)\n",
    "\n",
    "        # Layer 9\n",
    "        out = self.conv9(out)\n",
    "        out = self.bn9(out)\n",
    "        out = self.relu9(out)\n",
    "        out = self.pool9(out)\n",
    "\n",
    "        # Layer 10\n",
    "        out = self.conv10(out)\n",
    "        out = self.bn10(out)\n",
    "        out = self.relu10(out)\n",
    "        out = self.pool10(out)\n",
    "\n",
    "        # Layer 11\n",
    "        out = self.conv11(out)\n",
    "        out = self.bn11(out)\n",
    "        out = self.relu11(out)\n",
    "        out = self.pool11(out)\n",
    "\n",
    "        # Layer 12\n",
    "        out = self.conv12(out)\n",
    "        out = self.bn12(out)\n",
    "        out = self.relu12(out)\n",
    "        out = self.pool12(out)\n",
    "\n",
    "        # Flatten\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        out = self.fc(out)\n",
    "        out = self.relu13(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGEncoder(OneDimCNN):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ECGEncoder, self).__init__(num_classes)\n",
    "        self.fc3 = nn.Linear(128, 768)  # Change the input dimension to match the output of OneDimCNN\n",
    "\n",
    "    def forward(self, signal):  # Rename this method to `forward`\n",
    "        embedding = super().forward(signal)  # Call the parent class's `forward` method\n",
    "        embedding = self.fc3(embedding)  # Apply the new linear layer to the output of OneDimCNN\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of classes\n",
    "num_classes = 126  # Replace with the actual number of classes\n",
    "\n",
    "# Create an instance of the model\n",
    "ecg_encoder = ECGEncoder(num_classes)\n",
    "\n",
    "# Convert the numpy array to a PyTorch tensor\n",
    "input_data = torch.from_numpy(train_set[0][1]['val']).float()\n",
    "\n",
    "# Add an extra dimension for the batch size\n",
    "input_data = input_data.unsqueeze(0)\n",
    "\n",
    "# Convert the model's weights to Float\n",
    "ecg_encoder = ecg_encoder.float()\n",
    "\n",
    "# Pass the data through the model\n",
    "output = ecg_encoder(input_data)\n",
    "\n",
    "print(output.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
