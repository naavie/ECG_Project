{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Thesis Topic: “Zero-shot classification of ECG signals using CLIP-like model”. \n",
    "\n",
    "**For example: Train on PBT-XL:** \n",
    "\n",
    "- Text Encoder: ClinicalBERT (trained on diagnoses of ECG signal to obtain corresponding embeddings)\n",
    "- Image Encoder: 1D-CNN (used to encode ECG signal to obtain signal embeddings)\n",
    "\n",
    "- Experiment A): Baseline: We can take only the name of the class. For example, take “Myocardial Infarction” as a text. We should exclude some classes from training and after training is completed, the CLIP-like model can be tested on these excluded classes. \n",
    "    - Next, we get embeddings of text from ClinicalBERT and train the ECG encoder with contrastive loss.\n",
    "\n",
    "- Experiment B): Same as Experiment A but instead of testing on the same dataset/classes, we would test on other datasets containing different classes.\n",
    "\n",
    "**Evaluation metrics:** \n",
    "- Main: AUC-ROC, average_precison_score, \n",
    "- Optional: Specificity, Sensitivity, F1-score \n",
    "\n",
    "**Outcome:** \n",
    "- It’s possible to train CLIP-like models with freezed (or unchanged/not fine tuned for downstream tasks) text encoder\n",
    "- Training ECG encoders that are viable for representing different domains (within ECG modality) and previously unseen classes. \n",
    "- Training a CLIP-like model on ECGs has little novelty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Class ECG Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we preprocess the ECG data from the PhysioNet 2021 challenge dataset. This data will be loaded using the ```PhysioNetDataset``` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import ast\n",
    "import scipy.io as sio\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/navme/Desktop/ECG_Project/PyFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to dir/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training\n",
    "PhysioNet_PATH = f'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'\n",
    "PhysioNet_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the ```PhysioNet_PATH```, we can create separate datasets for training, testing & validation.\n",
    "\n",
    "# Data Preprocessing \n",
    "\n",
    "- train_set (train & validation data)\n",
    "- test_set (test data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65900, 22352)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = PhysioNetDataset(PhysioNet_PATH, train=True)\n",
    "test_set = PhysioNetDataset(PhysioNet_PATH, train=False)\n",
    "\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```train_set``` can be split into ```current_train``` and ```current_val```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for the random number generator\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Get the length of the train_set\n",
    "length = len(train_set)\n",
    "\n",
    "# Calculate the lengths of the splits\n",
    "train_length = int(0.85 * length)\n",
    "val_length = length - train_length\n",
    "\n",
    "# Split the dataset\n",
    "current_train, current_val = random_split(train_set, [train_length, val_length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to extract the header data for ```current_train```, ```current_val```, and ```test_set``` and save the data to a csv file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 56015/56015 [14:35<00:00, 63.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 56015 records.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the records\n",
    "records = []\n",
    "\n",
    "# Iterate over all records\n",
    "for i in tqdm(range(len(current_train)), desc=\"Processing records\"):\n",
    "    record, _ = train_set[i]  # Get the record (ignore the ECG data for now)\n",
    "    \n",
    "    # Flatten the 'leads_info' list into separate columns for each lead\n",
    "    for j, lead_info in enumerate(record['leads_info']):\n",
    "        for key, value in lead_info.items():\n",
    "            record[f'lead_{j}_{key}'] = value\n",
    "    del record['leads_info']  # We don't need the 'leads_info' list anymore\n",
    "\n",
    "    # Append the record to the list\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the list of records into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('train_set_records.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## current_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the records\n",
    "records = []\n",
    "\n",
    "# Iterate over all records\n",
    "for i in tqdm(range(len(current_val)), desc=\"Processing records\"):\n",
    "    record, _ = train_set[i]  # Get the record (ignore the ECG data for now)\n",
    "    \n",
    "    # Flatten the 'leads_info' list into separate columns for each lead\n",
    "    for j, lead_info in enumerate(record['leads_info']):\n",
    "        for key, value in lead_info.items():\n",
    "            record[f'lead_{j}_{key}'] = value\n",
    "    del record['leads_info']  # We don't need the 'leads_info' list anymore\n",
    "\n",
    "    # Append the record to the list\n",
    "    records.append(record)\n",
    "\n",
    "# Convert the list of records into a DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('train_set_records.csv', index=False)\n",
    "\n",
    "print(f\"Processed {len(records)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
