{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet 2021 Challenge\n",
    "\n",
    "The training data contains twelve-lead ECGs. The validation and test data contains twelve-lead, six-lead, four-lead, three-lead, and two-lead ECGs:\n",
    "\n",
    "1. Twelve leads: I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6\n",
    "2. Six leads: I, II, III, aVR, aVL, aVF\n",
    "3. Four leads: I, II, III, V2\n",
    "4. Three leads: I, II, V2\n",
    "5. Two leads: I, II\n",
    "\n",
    "Each ECG recording has one or more labels that describe cardiac abnormalities (and/or a normal sinus rhythm).\n",
    "\n",
    "The Challenge data include annotated twelve-lead ECG recordings from six sources in four countries across three continents. These databases include over 100,000 twelve-lead ECG recordings with over 88,000 ECGs shared publicly as training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a header file A0001.hea may have the following contents:\n",
    "\n",
    "```\n",
    "    A0001 12 500 7500\n",
    "    A0001.mat 16+24 1000/mV 16 0 28 -1716 0 I\n",
    "    A0001.mat 16+24 1000/mV 16 0 7 2029 0 II\n",
    "    A0001.mat 16+24 1000/mV 16 0 -21 3745 0 III\n",
    "    A0001.mat 16+24 1000/mV 16 0 -17 3680 0 aVR\n",
    "    A0001.mat 16+24 1000/mV 16 0 24 -2664 0 aVL\n",
    "    A0001.mat 16+24 1000/mV 16 0 -7 -1499 0 aVF\n",
    "    A0001.mat 16+24 1000/mV 16 0 -290 390 0 V1\n",
    "    A0001.mat 16+24 1000/mV 16 0 -204 157 0 V2\n",
    "    A0001.mat 16+24 1000/mV 16 0 -96 -2555 0 V3\n",
    "    A0001.mat 16+24 1000/mV 16 0 -112 49 0 V4\n",
    "    A0001.mat 16+24 1000/mV 16 0 -596 -321 0 V5\n",
    "    A0001.mat 16+24 1000/mV 16 0 -16 -3112 0 V6\n",
    "    #Age: 74\n",
    "    #Sex: Male\n",
    "    #Dx: 426783006\n",
    "    #Rx: Unknown\n",
    "    #Hx: Unknown\n",
    "    #Sx: Unknown\n",
    "```\n",
    "\n",
    "From the first line of the file:\n",
    "- We see that the recording number is A0001, and the recording file is A0001.mat. \n",
    "- The recording has 12 leads, each recorded at a 500 Hz sampling frequency, and contains 7500 samples. \n",
    "- From the next 12 lines of the file (one for each lead), we see that each signal:\n",
    "    - Was written at 16 bits with an offset of 24 bits\n",
    "    - The floating point number (analog-to-digital converter (ADC) units per physical unit) is 1000/mV \n",
    "    - The resolution of the analog-to-digital converter (ADC) used to digitize the signal is 16 bits, and the baseline value corresponding to 0 physical units is 0. \n",
    "    - The first value of the signal (-1716, etc.), the checksum (0, etc.), and the lead name (I, etc.) are the last three entries of each of these lines. \n",
    "- From the final 6 lines, we see that the patient is:\n",
    "    - A 74-year-old male \n",
    "    - With a diagnosis (Dx) of 426783006, which is the **SNOMED-CT code** for sinus rhythm. \n",
    "    - The medical prescription (Rx), history (Hx), and symptom or surgery (Sx) are unknown. \n",
    "\n",
    "- Please visit WFDB header format for more information on the header file and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/navme/Desktop/ECG_Project/PyFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PhysioNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PhysioNet_PATH = f'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'\n",
    "PhysioNet_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val/Test PhysioNet Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_set = PhysioNetDataset(PhysioNet_PATH, train=True)\n",
    "\n",
    "# Val\n",
    "val_set = PhysioNetDataset(PhysioNet_PATH, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of ECG header data\n",
    "train_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of ECG signal\n",
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Processed Patient Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PhysioNet 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df_path = r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\processed_train_set_records.csv'\n",
    "processed_val_df_path = r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\processed_val_set_records.csv'\n",
    "\n",
    "# Fix URL formatting\n",
    "processed_train_df_path = convert_to_forward_slashes(processed_train_df_path)\n",
    "processed_val_df_path = convert_to_forward_slashes(processed_val_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df = pd.read_csv(processed_train_df_path)\n",
    "processed_val_df = pd.read_csv(processed_val_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE15_df_path = r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\CODE-15\\exams.csv'\n",
    "\n",
    "# Fix URL formatting\n",
    "CODE15_df_path = convert_to_forward_slashes(CODE15_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE15_df = pd.read_csv(CODE15_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE15_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextEncoder()\n",
    "\n",
    "Create a class, ```TextEncoder()``` that is used to convert the description of the (dx_modality) diagnosis class into embeddings using the ClinicalBERT model.\n",
    "\n",
    "- Input should be a concatenated using comma or blank space string of diagnoses/dx_modality per ECG signal.\n",
    "- Use processed CSV files (dx_modality vs dx_modality, age, etc together)\n",
    "- Frozen weights (since it's already pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhysioNet Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: dx_modality only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "    def encode(self, text_list):\n",
    "        # Check if text_list is a string representation of a list\n",
    "        if isinstance(text_list, str):\n",
    "            text_list = ast.literal_eval(text_list)\n",
    "        # Convert list of strings to a single string\n",
    "        text = ', '.join(text_list)\n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        # Get embeddings from ClinicalBERT model\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state\n",
    "        # Average the embeddings to get single vector per each input\n",
    "        embeddings = torch.mean(embeddings, dim=1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(processed_train_df['dx_modality'][4], str):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of TextEncoder\n",
    "encoder = TextEncoder()\n",
    "embeddings = encoder.encode(processed_train_df['dx_modality'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of the embeddings\n",
    "print(embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: dx_modality plus age, sex, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "    def encode(self, series):\n",
    "        text = f\"{series['age']}, {series['sex']}, {series['dx_modality']}\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state\n",
    "        embeddings = torch.mean(embeddings, dim=1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TextEncoder()\n",
    "embeddings = encoder.encode(processed_train_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE-15 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECGEncoder() \n",
    "\n",
    "- Input is ECG signal, output will be embeddings of ECG signal\n",
    "- This is going to be model in model.py \n",
    "- Model weights are updated iteratively\n",
    "- optimizer = torch.optim.Adam(clip_model.ECGEncoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import OneDimCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OneDimCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(OneDimCNN, self).__init__()\n",
    "\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 3\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 4\n",
    "        self.conv4 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully Connected Layer 1\n",
    "        self.fc1 = nn.Linear(79872, 500)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully Connected Layer 2\n",
    "        self.fc2 = nn.Linear(500, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Layer 4\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(x.shape)  # Add this line\n",
    "\n",
    "        # Fully Connected Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Fully Connected Layer 2\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGEncoder(OneDimCNN):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ECGEncoder, self).__init__(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_set[0][1]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0][1]['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 79872])\n",
      "tensor([[-4.1234e-01,  6.3279e-02,  1.5875e-01, -2.9944e-01,  9.9462e-02,\n",
      "          6.3645e-02, -2.3087e-01, -1.2965e-01,  2.4169e-01,  1.7064e-01,\n",
      "          1.8912e-01,  3.7561e-01, -2.3039e-01, -1.0117e-01, -4.3106e-02,\n",
      "         -1.4789e-02, -4.1741e-02,  4.7373e-01, -3.4525e-01,  3.8595e-01,\n",
      "          2.1732e-01,  3.8935e-01, -1.3965e-01, -3.0145e-02, -1.1811e-01,\n",
      "          7.4810e-03,  3.3934e-03, -4.6897e-02,  3.1299e-01, -1.0522e-01,\n",
      "         -1.5537e-01,  2.5957e-02, -5.1449e-04,  3.9885e-01, -2.8495e-01,\n",
      "         -1.8094e-01,  1.0541e-01,  3.9707e-02, -6.4283e-01, -2.9106e-01,\n",
      "          3.5229e-01, -1.8506e-02, -2.8062e-01,  3.2971e-02,  5.1607e-02,\n",
      "          1.7201e-01,  1.4050e-01,  4.0031e-01,  1.7801e-01,  2.6229e-01,\n",
      "          7.4191e-02,  4.2768e-02, -4.1430e-02,  7.9764e-02,  1.8574e-01,\n",
      "          3.8530e-02,  1.6014e-01, -1.8698e-01,  1.8816e-01,  8.1235e-02,\n",
      "         -1.1675e-01, -3.3772e-02,  5.5602e-01,  2.2724e-01,  3.1683e-01,\n",
      "          2.8079e-01,  3.2294e-01, -9.0442e-02,  5.2962e-02,  3.3204e-04,\n",
      "         -2.7632e-01,  4.2208e-03, -2.9286e-01, -6.3202e-01,  1.8188e-01,\n",
      "         -2.1059e-01,  3.1467e-01,  7.8310e-02, -2.4149e-02, -1.9933e-02,\n",
      "          2.5194e-01, -3.7317e-02,  1.5589e-01,  1.6688e-01, -6.9953e-02,\n",
      "          8.2834e-02, -3.3774e-01, -3.2189e-01, -2.5116e-01,  3.0721e-03,\n",
      "         -2.2845e-01, -2.8790e-01,  2.2021e-01,  5.2562e-02,  2.0531e-01,\n",
      "          2.2132e-01, -1.2793e-01,  2.8820e-01,  1.8088e-01, -1.8181e-02,\n",
      "          3.8451e-01, -3.1372e-01,  2.2164e-01,  5.3162e-01, -3.4789e-01,\n",
      "          3.0305e-01,  1.6233e-01, -2.1447e-02, -1.9410e-01, -3.1023e-02,\n",
      "          4.0957e-04,  1.9114e-01, -1.9601e-01, -1.4818e-01,  7.1391e-02,\n",
      "          2.8079e-01,  1.5534e-01, -4.8161e-02, -1.1286e-01, -4.3729e-02,\n",
      "         -7.7964e-02, -6.4497e-02, -4.4292e-03, -2.5812e-01, -3.2640e-01,\n",
      "         -3.0780e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the number of classes\n",
    "num_classes = 126  # Replace with the actual number of classes\n",
    "\n",
    "# Create an instance of the model\n",
    "ecg_encoder = ECGEncoder(num_classes)\n",
    "\n",
    "# Convert the numpy array to a PyTorch tensor\n",
    "input_data = torch.from_numpy(train_set[60000][1]['val']).float()\n",
    "\n",
    "# Add an extra dimension for the batch size\n",
    "input_data = input_data.unsqueeze(0)\n",
    "\n",
    "# Convert the model's weights to Float\n",
    "ecg_encoder = ecg_encoder.float()\n",
    "\n",
    "# Pass the data through the model\n",
    "output = ecg_encoder(input_data)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
