{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhysioNet 2021 Challenge\n",
    "\n",
    "The training data contains twelve-lead ECGs. The validation and test data contains twelve-lead, six-lead, four-lead, three-lead, and two-lead ECGs:\n",
    "\n",
    "1. Twelve leads: I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6\n",
    "2. Six leads: I, II, III, aVR, aVL, aVF\n",
    "3. Four leads: I, II, III, V2\n",
    "4. Three leads: I, II, V2\n",
    "5. Two leads: I, II\n",
    "\n",
    "Each ECG recording has one or more labels that describe cardiac abnormalities (and/or a normal sinus rhythm).\n",
    "\n",
    "The Challenge data include annotated twelve-lead ECG recordings from six sources in four countries across three continents. These databases include over 100,000 twelve-lead ECG recordings with over 88,000 ECGs shared publicly as training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, a header file A0001.hea may have the following contents:\n",
    "\n",
    "```\n",
    "    A0001 12 500 7500\n",
    "    A0001.mat 16+24 1000/mV 16 0 28 -1716 0 I\n",
    "    A0001.mat 16+24 1000/mV 16 0 7 2029 0 II\n",
    "    A0001.mat 16+24 1000/mV 16 0 -21 3745 0 III\n",
    "    A0001.mat 16+24 1000/mV 16 0 -17 3680 0 aVR\n",
    "    A0001.mat 16+24 1000/mV 16 0 24 -2664 0 aVL\n",
    "    A0001.mat 16+24 1000/mV 16 0 -7 -1499 0 aVF\n",
    "    A0001.mat 16+24 1000/mV 16 0 -290 390 0 V1\n",
    "    A0001.mat 16+24 1000/mV 16 0 -204 157 0 V2\n",
    "    A0001.mat 16+24 1000/mV 16 0 -96 -2555 0 V3\n",
    "    A0001.mat 16+24 1000/mV 16 0 -112 49 0 V4\n",
    "    A0001.mat 16+24 1000/mV 16 0 -596 -321 0 V5\n",
    "    A0001.mat 16+24 1000/mV 16 0 -16 -3112 0 V6\n",
    "    #Age: 74\n",
    "    #Sex: Male\n",
    "    #Dx: 426783006\n",
    "    #Rx: Unknown\n",
    "    #Hx: Unknown\n",
    "    #Sx: Unknown\n",
    "```\n",
    "\n",
    "From the first line of the file:\n",
    "- We see that the recording number is A0001, and the recording file is A0001.mat. \n",
    "- The recording has 12 leads, each recorded at a 500 Hz sampling frequency, and contains 7500 samples. \n",
    "- From the next 12 lines of the file (one for each lead), we see that each signal:\n",
    "    - Was written at 16 bits with an offset of 24 bits\n",
    "    - The floating point number (analog-to-digital converter (ADC) units per physical unit) is 1000/mV \n",
    "    - The resolution of the analog-to-digital converter (ADC) used to digitize the signal is 16 bits, and the baseline value corresponding to 0 physical units is 0. \n",
    "    - The first value of the signal (-1716, etc.), the checksum (0, etc.), and the lead name (I, etc.) are the last three entries of each of these lines. \n",
    "- From the final 6 lines, we see that the patient is:\n",
    "    - A 74-year-old male \n",
    "    - With a diagnosis (Dx) of 426783006, which is the **SNOMED-CT code** for sinus rhythm. \n",
    "    - The medical prescription (Rx), history (Hx), and symptom or surgery (Sx) are unknown. \n",
    "\n",
    "- Please visit WFDB header format for more information on the header file and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import resample\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import ast\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/navme/Desktop/ECG_Project/PyFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PhysioNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PhysioNet_PATH = f'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'\n",
    "PhysioNet_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val/Test PhysioNet Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_set = PhysioNetDataset(PhysioNet_PATH, train=True)\n",
    "\n",
    "# Val\n",
    "val_set = PhysioNetDataset(PhysioNet_PATH, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of ECG header data\n",
    "train_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of ECG signal\n",
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Processed Patient Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PhysioNet 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df_path = r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\processed_train_set_records.csv'\n",
    "processed_val_df_path = r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\PhysioNet\\processed_val_set_records.csv'\n",
    "\n",
    "# Fix URL formatting\n",
    "processed_train_df_path = convert_to_forward_slashes(processed_train_df_path)\n",
    "processed_val_df_path = convert_to_forward_slashes(processed_val_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df = pd.read_csv(processed_train_df_path)\n",
    "processed_val_df = pd.read_csv(processed_val_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE15_df_path = r'C:\\Users\\navme\\Desktop\\ECG_Project\\Data\\CODE-15\\exams.csv'\n",
    "\n",
    "# Fix URL formatting\n",
    "CODE15_df_path = convert_to_forward_slashes(CODE15_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE15_df = pd.read_csv(CODE15_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exam_id</th>\n",
       "      <th>age</th>\n",
       "      <th>is_male</th>\n",
       "      <th>nn_predicted_age</th>\n",
       "      <th>1dAVb</th>\n",
       "      <th>RBBB</th>\n",
       "      <th>LBBB</th>\n",
       "      <th>SB</th>\n",
       "      <th>ST</th>\n",
       "      <th>AF</th>\n",
       "      <th>patient_id</th>\n",
       "      <th>death</th>\n",
       "      <th>timey</th>\n",
       "      <th>normal_ecg</th>\n",
       "      <th>trace_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1169160</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "      <td>40.160484</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>523632</td>\n",
       "      <td>False</td>\n",
       "      <td>2.098628</td>\n",
       "      <td>True</td>\n",
       "      <td>exams_part13.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2873686</td>\n",
       "      <td>73</td>\n",
       "      <td>True</td>\n",
       "      <td>67.059440</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1724173</td>\n",
       "      <td>False</td>\n",
       "      <td>6.657529</td>\n",
       "      <td>False</td>\n",
       "      <td>exams_part13.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168405</td>\n",
       "      <td>67</td>\n",
       "      <td>True</td>\n",
       "      <td>79.621740</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>51421</td>\n",
       "      <td>False</td>\n",
       "      <td>4.282188</td>\n",
       "      <td>False</td>\n",
       "      <td>exams_part13.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>271011</td>\n",
       "      <td>41</td>\n",
       "      <td>True</td>\n",
       "      <td>69.750260</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1737282</td>\n",
       "      <td>False</td>\n",
       "      <td>4.038353</td>\n",
       "      <td>True</td>\n",
       "      <td>exams_part13.hdf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>384368</td>\n",
       "      <td>73</td>\n",
       "      <td>True</td>\n",
       "      <td>78.873460</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>331652</td>\n",
       "      <td>False</td>\n",
       "      <td>3.786298</td>\n",
       "      <td>False</td>\n",
       "      <td>exams_part13.hdf5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   exam_id  age  is_male  nn_predicted_age  1dAVb   RBBB   LBBB     SB     ST  \\\n",
       "0  1169160   38     True         40.160484  False  False  False  False  False   \n",
       "1  2873686   73     True         67.059440  False  False  False  False  False   \n",
       "2   168405   67     True         79.621740  False  False  False  False  False   \n",
       "3   271011   41     True         69.750260  False  False  False  False  False   \n",
       "4   384368   73     True         78.873460  False  False  False  False  False   \n",
       "\n",
       "      AF  patient_id  death     timey  normal_ecg         trace_file  \n",
       "0  False      523632  False  2.098628        True  exams_part13.hdf5  \n",
       "1  False     1724173  False  6.657529       False  exams_part13.hdf5  \n",
       "2   True       51421  False  4.282188       False  exams_part13.hdf5  \n",
       "3  False     1737282  False  4.038353        True  exams_part13.hdf5  \n",
       "4  False      331652  False  3.786298       False  exams_part13.hdf5  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CODE15_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextEncoder()\n",
    "\n",
    "Create a class, ```TextEncoder()``` that is used to convert the description of the (dx_modality) diagnosis class into embeddings using the ClinicalBERT model.\n",
    "\n",
    "- Input should be a concatenated using comma or blank space string of diagnoses/dx_modality per ECG signal.\n",
    "- Use processed CSV files (dx_modality vs dx_modality, age, etc together)\n",
    "- Frozen weights (since it's already pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhysioNet Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: dx_modality only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "    def encode(self, text_list):\n",
    "        # Check if text_list is a string representation of a list\n",
    "        if isinstance(text_list, str):\n",
    "            text_list = ast.literal_eval(text_list)\n",
    "        # Convert list of strings to a single string\n",
    "        text = ', '.join(text_list)\n",
    "        # Tokenize text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        # Get embeddings from ClinicalBERT model\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state\n",
    "        # Average the embeddings to get single vector per each input\n",
    "        embeddings = torch.mean(embeddings, dim=1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if isinstance(processed_train_df['dx_modality'][4], str):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of TextEncoder\n",
    "encoder = TextEncoder()\n",
    "embeddings = encoder.encode(processed_train_df['dx_modality'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of the embeddings\n",
    "print(embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: dx_modality plus age, sex, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "    def encode(self, series):\n",
    "        text = f\"{series['age']}, {series['sex']}, {series['dx_modality']}\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs).last_hidden_state\n",
    "        embeddings = torch.mean(embeddings, dim=1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TextEncoder()\n",
    "embeddings = encoder.encode(processed_train_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE-15 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECGEncoder() \n",
    "\n",
    "- Input is ECG signal, output will be embeddings of ECG signal\n",
    "- This is going to be model in model.py \n",
    "- Model weights are updated iteratively\n",
    "- optimizer = torch.optim.Adam(clip_model.ECGEncoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import OneDimCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class OneDimCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(OneDimCNN, self).__init__()\n",
    "\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 3\n",
    "        self.conv3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 4\n",
    "        self.conv4 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.pool4 = nn.AvgPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Fully Connected Layer 1\n",
    "        self.fc1 = nn.Linear(79872, 128)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        # Fully Connected Layer 2\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        # Layer 4\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "\n",
    "        # Flatten the tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)  # Add this line\n",
    "\n",
    "        # Fully Connected Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Fully Connected Layer 2\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGEncoder(OneDimCNN):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ECGEncoder, self).__init__(num_classes)\n",
    "        self.fc3 = nn.Linear(126, 768)  # New linear layer\n",
    "\n",
    "    def encode(self, signal):\n",
    "        signal = torch.tensor(signal, dtype=torch.float).unsqueeze(0)\n",
    "        embedding = self.forward(signal)\n",
    "        return self.fc3(embedding)  # Apply the new linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_set[0][1]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[408.24601882, 408.24601882, 408.24601882, ..., -83.34581329,\n",
       "        -74.965045  , -63.10339951],\n",
       "       [-92.07603073, -92.07603073, -92.07603073, ...,  57.20010276,\n",
       "         54.51591647,  58.88514819],\n",
       "       [225.08001192, 225.08001192, 225.08001192, ...,  93.39571052,\n",
       "         97.44912853, 117.96825132]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][1]['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 79872])\n",
      "tensor([[-1.1187e-01,  2.3097e-01, -1.3781e-01, -1.5903e-01,  3.5233e-01,\n",
      "          9.1446e-02,  2.3288e-01,  3.6046e-01, -4.6508e-02, -3.3814e-01,\n",
      "         -6.0480e-01,  1.3641e-01,  2.7077e-01,  2.8557e-02,  4.2393e-01,\n",
      "         -3.3274e-01,  4.7674e-02, -4.7889e-02, -1.2955e-01, -2.0932e-01,\n",
      "         -6.5117e-02,  4.4930e-01, -3.0702e-01,  1.4360e-04, -1.7969e-01,\n",
      "         -6.3248e-02,  1.7239e-01, -1.8230e-02,  7.2395e-02, -3.5511e-01,\n",
      "          2.4709e-01, -4.2690e-02, -2.2465e-01, -2.5672e-01, -2.0102e-01,\n",
      "          9.7384e-02,  9.3277e-02,  9.4923e-02, -4.2708e-01,  2.2382e-01,\n",
      "          1.6069e-01, -1.4382e-01, -1.5387e-01, -1.6791e-01, -1.8936e-01,\n",
      "         -2.4707e-01, -1.4342e-02, -6.3892e-02, -2.4525e-01,  2.0876e-01,\n",
      "         -2.9727e-01, -3.7843e-01, -1.7891e-01,  2.3789e-01, -3.1652e-01,\n",
      "         -3.5186e-01,  1.1949e-01,  2.4470e-01,  3.9233e-01,  3.5625e-01,\n",
      "          2.4722e-01,  1.1365e-01, -1.3010e-01, -2.0656e-01, -4.8631e-01,\n",
      "         -2.7027e-01,  4.2922e-02, -5.2259e-02,  3.6590e-01,  2.2601e-01,\n",
      "         -1.5711e-01,  3.4512e-01, -2.2463e-01,  4.2130e-02,  2.5535e-02,\n",
      "         -2.5274e-01, -1.9482e-02, -2.6627e-01,  3.3810e-01,  2.6953e-01,\n",
      "         -5.4269e-02, -2.9052e-02,  8.6330e-02, -1.3507e-01, -1.2503e-01,\n",
      "          3.4420e-01,  1.6653e-01,  1.1920e-01, -1.5446e-01,  1.3081e-01,\n",
      "          2.4046e-01,  3.1308e-01, -2.1084e-01, -2.6178e-01, -6.5643e-02,\n",
      "          5.4039e-02,  2.6171e-01, -5.9949e-01,  4.7651e-01,  3.1959e-01,\n",
      "          3.3761e-01,  3.1477e-01, -7.4208e-02,  1.3292e-01, -3.8985e-02,\n",
      "         -3.0592e-01,  1.0581e-01, -1.9634e-01,  4.1083e-01,  2.7726e-01,\n",
      "         -2.5026e-02,  1.7590e-01,  3.5683e-02, -2.9089e-02, -1.1538e-01,\n",
      "          5.6429e-02, -4.4356e-01,  3.2889e-01, -5.2852e-01,  2.9367e-01,\n",
      "          3.0384e-01,  2.4017e-02, -2.2813e-01, -1.6022e-01, -3.5986e-02,\n",
      "         -7.3399e-02]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the number of classes\n",
    "num_classes = 126  # Replace with the actual number of classes\n",
    "\n",
    "# Create an instance of the model\n",
    "ecg_encoder = ECGEncoder(num_classes)\n",
    "\n",
    "# Convert the numpy array to a PyTorch tensor\n",
    "input_data = torch.from_numpy(train_set[60000][1]['val']).float()\n",
    "\n",
    "# Add an extra dimension for the batch size\n",
    "input_data = input_data.unsqueeze(0)\n",
    "\n",
    "# Convert the model's weights to Float\n",
    "ecg_encoder = ecg_encoder.float()\n",
    "\n",
    "# Pass the data through the model\n",
    "output = ecg_encoder(input_data)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 79872])\n",
      "tensor([[-1.3415e-01,  4.2046e-02, -2.3089e-02, -9.6867e-02,  5.6141e-02,\n",
      "         -3.6813e-02,  2.3188e-02,  5.6476e-02, -6.9348e-03, -1.4045e-01,\n",
      "         -1.4861e-01,  5.0855e-02,  1.2150e-01, -3.3249e-02, -5.2528e-02,\n",
      "         -4.0292e-02, -9.3183e-02,  9.1527e-02, -5.8979e-02, -7.0683e-03,\n",
      "         -3.0763e-02,  4.0828e-02, -7.4873e-02, -3.9866e-02, -6.1620e-02,\n",
      "          7.2322e-02, -1.0775e-02, -5.0234e-02,  8.3719e-03, -3.5085e-02,\n",
      "          5.0136e-03,  7.3070e-02, -3.1885e-02,  5.0698e-02, -4.5149e-02,\n",
      "         -7.1235e-02,  9.1999e-02,  8.6871e-02, -4.2471e-02,  1.1170e-01,\n",
      "          6.7944e-02, -3.8681e-02, -8.1415e-02, -6.4703e-02, -7.4312e-02,\n",
      "          5.1688e-02,  4.9098e-02,  7.3638e-02, -4.9833e-02,  7.8834e-02,\n",
      "         -6.6390e-02, -2.9900e-02,  4.8231e-03,  7.2578e-02,  7.4812e-03,\n",
      "         -2.7810e-03,  2.1301e-02,  2.5543e-02,  7.5328e-02,  5.3012e-02,\n",
      "          5.9585e-02,  2.1722e-02,  5.6241e-02, -6.9900e-02, -1.2700e-01,\n",
      "         -7.7793e-02, -7.3267e-02, -2.5245e-02,  2.1635e-02,  3.6357e-02,\n",
      "         -2.6037e-02,  1.6971e-01, -5.6414e-02, -3.3228e-02, -3.1416e-02,\n",
      "         -4.8826e-02, -1.1179e-02,  3.2711e-02,  5.6788e-02,  9.0168e-02,\n",
      "         -8.5132e-02,  1.9065e-02,  3.5161e-02,  6.5491e-02, -9.6791e-02,\n",
      "          7.8310e-02,  1.2739e-01, -5.4975e-02, -1.3345e-02,  1.8371e-02,\n",
      "          8.7730e-02, -2.1015e-02,  3.5748e-02, -1.1734e-01, -6.7906e-02,\n",
      "          4.4991e-03,  9.6863e-02, -4.2762e-02,  9.0135e-02,  6.7079e-02,\n",
      "          1.3800e-01,  4.3827e-02,  2.1406e-02, -1.0150e-01, -8.7347e-02,\n",
      "         -6.9275e-02,  1.0886e-01,  1.6002e-02,  1.0219e-01, -4.4940e-02,\n",
      "          5.6709e-02,  4.8615e-02, -8.8143e-03, -9.6948e-03, -1.6110e-02,\n",
      "          5.3001e-02, -8.9817e-02, -2.1897e-05, -9.6707e-02,  1.0428e-01,\n",
      "          3.9665e-02, -8.0440e-02, -7.9803e-02, -7.7091e-03,  5.9423e-03,\n",
      "          6.9424e-02]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Convert the model's weights to Float\n",
    "ecg_encoder = ecg_encoder.float()\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "ecg_encoder.eval()\n",
    "\n",
    "# Pass the data through the model\n",
    "output = ecg_encoder(input_data)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update TripletLoss() such that:\n",
    "\n",
    "- positive_instances are where the ECG embedding and dx_modality embedding align (from the same file/reading)\n",
    "- negative_instances are where these two embeddings do not align\n",
    "- filter out text embeddings that are the same or equal to the positive_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording_number</th>\n",
       "      <th>recording_file</th>\n",
       "      <th>num_leads</th>\n",
       "      <th>sampling_frequency</th>\n",
       "      <th>num_samples</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>dx</th>\n",
       "      <th>rx</th>\n",
       "      <th>hx</th>\n",
       "      <th>...</th>\n",
       "      <th>lead_10_lead_name</th>\n",
       "      <th>lead_11_file</th>\n",
       "      <th>lead_11_adc_gain</th>\n",
       "      <th>lead_11_units</th>\n",
       "      <th>lead_11_adc_resolution</th>\n",
       "      <th>lead_11_adc_zero</th>\n",
       "      <th>lead_11_initial_value</th>\n",
       "      <th>lead_11_checksum</th>\n",
       "      <th>lead_11_lead_name</th>\n",
       "      <th>dx_modality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JS00001</td>\n",
       "      <td>JS00001.mat</td>\n",
       "      <td>12</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>['164889003', '59118001', '164934002']</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>JS00001.mat</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>mV</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>527</td>\n",
       "      <td>32579</td>\n",
       "      <td>0</td>\n",
       "      <td>['atrial fibrillation', 'right bundle branch b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JS00002</td>\n",
       "      <td>JS00002.mat</td>\n",
       "      <td>12</td>\n",
       "      <td>500</td>\n",
       "      <td>5000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>['426177001', '164934002']</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>JS00002.mat</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>mV</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31542</td>\n",
       "      <td>0</td>\n",
       "      <td>['sinus bradycardia', 't wave abnormal']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  recording_number recording_file  num_leads  sampling_frequency  num_samples  \\\n",
       "0          JS00001    JS00001.mat         12                 500         5000   \n",
       "1          JS00002    JS00002.mat         12                 500         5000   \n",
       "\n",
       "    age     sex                                      dx       rx       hx  \\\n",
       "0  85.0    Male  ['164889003', '59118001', '164934002']  Unknown  Unknown   \n",
       "1  59.0  Female              ['426177001', '164934002']  Unknown  Unknown   \n",
       "\n",
       "   ... lead_10_lead_name lead_11_file  lead_11_adc_gain lead_11_units  \\\n",
       "0  ...                 0  JS00001.mat            1000.0            mV   \n",
       "1  ...                 0  JS00002.mat            1000.0            mV   \n",
       "\n",
       "   lead_11_adc_resolution  lead_11_adc_zero  lead_11_initial_value  \\\n",
       "0                      16                 0                    527   \n",
       "1                      16                 0                      0   \n",
       "\n",
       "   lead_11_checksum  lead_11_lead_name  \\\n",
       "0             32579                  0   \n",
       "1             31542                  0   \n",
       "\n",
       "                                         dx_modality  \n",
       "0  ['atrial fibrillation', 'right bundle branch b...  \n",
       "1           ['sinus bradycardia', 't wave abnormal']  \n",
       "\n",
       "[2 rows x 108 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recording_number': 'JS00001',\n",
       " 'recording_file': 'JS00001.mat',\n",
       " 'num_leads': 12,\n",
       " 'sampling_frequency': 500,\n",
       " 'num_samples': 5000,\n",
       " 'leads_info': [{'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -254,\n",
       "   'checksum': 21756,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 264,\n",
       "   'checksum': -599,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 517,\n",
       "   'checksum': -22376,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -5,\n",
       "   'checksum': 28232,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -386,\n",
       "   'checksum': 16619,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 390,\n",
       "   'checksum': 15121,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -98,\n",
       "   'checksum': 1568,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -312,\n",
       "   'checksum': -32761,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -98,\n",
       "   'checksum': 32715,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 810,\n",
       "   'checksum': 15193,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 810,\n",
       "   'checksum': 14081,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 527,\n",
       "   'checksum': 32579,\n",
       "   'lead_name': '0'}],\n",
       " 'age': 85,\n",
       " 'sex': 'Male',\n",
       " 'dx': ['164889003', '59118001', '164934002'],\n",
       " 'rx': 'Unknown',\n",
       " 'hx': 'Unknown',\n",
       " 'sx': 'Unknown'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "\n",
    "class TripletLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        positive_distance = F.pairwise_distance(anchor, positive, keepdim=True)\n",
    "        negative_distance = F.pairwise_distance(anchor, negative, keepdim=True)\n",
    "\n",
    "        triplet_loss = torch.mean(torch.clamp(positive_distance - negative_distance + self.margin, min=0.0))\n",
    "\n",
    "        return triplet_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I have a function that calulcates ClinicalBERT embedding of ECG header, class TextEncoder. \n",
    "\n",
    "I use this class to get ClinicalBERT embeddings of the dx_modality column. \n",
    "\n",
    "Next, I Have another class, ECGEncoder, which uses moden OneDimCNN to get ECG embeddings from a Physionet object. \n",
    "\n",
    "train_set[0][1]['val'] --> RETURNS THE ECG SIGNAL\n",
    "\n",
    "train_set[0][1] --< RETURSN THE ECG HEADER. \n",
    "\n",
    "I need to write a function that does the following: \n",
    "\n",
    "- It finds positive_instances where the ECG embedding (train_set[0][1]['val']) and dx_modality (processed_train_df['dx_modality'][0])) align (from the same file/reading)\n",
    "- negative_instances are where these two embeddings do not align\n",
    "\n",
    "FINALLY, - filter out text embeddings that are the same or equal to the positive_instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceSelector:\n",
    "    def __init__(self, train_set, processed_train_df, text_encoder, ecg_encoder):\n",
    "        self.train_set = train_set\n",
    "        self.processed_train_df = processed_train_df\n",
    "        self.text_encoder = text_encoder\n",
    "        self.ecg_encoder = ecg_encoder\n",
    "\n",
    "    def get_positive_instances(self):\n",
    "        positive_instances = []\n",
    "        for i in range(len(self.train_set)):\n",
    "            # Generate ECG embedding for the current instance in the training set\n",
    "            ecg_embedding = self.ecg_encoder.encode(self.train_set[i][1]['val'])\n",
    "            # Generate dx_modality embedding for the current instance in the processed DataFrame\n",
    "            dx_modality_embedding = self.text_encoder.encode(self.processed_train_df['dx_modality'][i])\n",
    "            # If the ECG embedding and dx_modality embedding are equal, append them as a positive instance\n",
    "            if torch.all(torch.eq(ecg_embedding, dx_modality_embedding)):\n",
    "                positive_instances.append((ecg_embedding, dx_modality_embedding))\n",
    "        return positive_instances\n",
    "\n",
    "    def get_negative_instances(self):\n",
    "        negative_instances = []\n",
    "        # Get the positive instances\n",
    "        positive_instances = self.get_positive_instances()\n",
    "        for i in range(len(self.train_set)):\n",
    "            # Generate ECG embedding for the current instance in the training set\n",
    "            ecg_embedding = self.ecg_encoder.encode(self.train_set[i][1]['val'])\n",
    "            for j in range(len(self.processed_train_df)):\n",
    "                # Only consider dx_modality embeddings that are not at the same index as the current ECG embedding\n",
    "                if i != j:\n",
    "                    # Generate dx_modality embedding for the current instance in the processed DataFrame\n",
    "                    dx_modality_embedding = self.text_encoder.encode(self.processed_train_df['dx_modality'][j])\n",
    "                    # If the ECG embedding does not match any of the positive instance embeddings, append it as a negative instance\n",
    "                    if not any(torch.all(torch.eq(ecg_embedding, pos[1])) for pos in positive_instances):\n",
    "                        negative_instances.append((ecg_embedding, dx_modality_embedding))\n",
    "        return negative_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder()\n",
    "ecg_encoder = ECGEncoder(num_classes=126)  # Assuming you have this class defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_selector = InstanceSelector(train_set, processed_train_df, text_encoder, ecg_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_instances = instance_selector.get_positive_instances()\n",
    "negative_instances = instance_selector.get_negative_instances()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, train_set, processed_train_df):\n",
    "        super(CLIPModel, self).__init__()\n",
    "        self.text_encoder = TextEncoder()  # Initialize TextEncoder\n",
    "        self.ecg_encoder = ECGEncoder(num_classes=126)  # Initialize ECGEncoder\n",
    "        self.instance_selector = InstanceSelector(train_set, processed_train_df, self.text_encoder, self.ecg_encoder)\n",
    "\n",
    "    def forward(self, ecg_signal, dx_modality):\n",
    "        ecg_embedding = self.ecg_encoder.encode(ecg_signal)\n",
    "        dx_modality_embedding = self.text_encoder.encode(dx_modality)\n",
    "        # Compute similarity between embeddings, e.g., using cosine similarity\n",
    "        similarity = F.cosine_similarity(ecg_embedding, dx_modality_embedding)\n",
    "        return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, train_set, processed_train_df):\n",
    "        super(CLIPModel, self).__init__()\n",
    "        self.ecg_encoder = ECGEncoder(num_classes=126)  # Initialize ECGEncoder\n",
    "        self.text_encoder = TextEncoder()  # Initialize TextEncoder\n",
    "        self.instance_selector = InstanceSelector(train_set, processed_train_df, self.text_encoder, self.ecg_encoder)\n",
    "\n",
    "    def forward(self, ecgs, diagnoses):\n",
    "        ecgs_embeddings = self.ecg_encoder(ecgs)\n",
    "        diagnoses_embeddings = self.text_encoder.encode(diagnoses)\n",
    "        positive_instances = self.instance_selector.get_positive_instances()\n",
    "        negative_instances = self.instance_selector.get_negative_instances()\n",
    "        # Compute loss based on whether the pair of embeddings is a positive or negative instance\n",
    "        loss = sum(F.cosine_similarity(ecgs_embeddings[i], diagnoses_embeddings[i]) for i in range(len(ecgs)) if (ecgs_embeddings[i], diagnoses_embeddings[i]) in positive_instances) \\\n",
    "             - sum(F.cosine_similarity(ecgs_embeddings[i], diagnoses_embeddings[i]) for i in range(len(ecgs)) if (ecgs_embeddings[i], diagnoses_embeddings[i]) in negative_instances)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PhysioNetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, train=False):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset_path = [path for path in self.dataset_path if \"index.html\" not in path]\n",
    "        self.train = train\n",
    "        self.file_list = os.listdir(dataset_path)\n",
    "        self._hea_files = []\n",
    "        self._mat_files = []\n",
    "        self._indices_files = []\n",
    "        self._hea_files_path = []\n",
    "        self._mat_files_path = []\n",
    "\n",
    "        self.file_PATHS = []  # Directory to main database folders\n",
    "        self.data_files = []  # Directory to data files\n",
    "\n",
    "        # Validation Case: PTB Databases only\n",
    "        if self.train == False:\n",
    "            validation_datasets = ['ptb', 'ptb-xl']\n",
    "            for file in os.listdir(dataset_path):\n",
    "                if file in validation_datasets:\n",
    "                    file_path = os.path.join(dataset_path, file)\n",
    "                    file_path = file_path.replace('\\\\', '/')\n",
    "                    self.file_PATHS.append(file_path)\n",
    "\n",
    "        # Training Case: All Databases excluding PTB\n",
    "        else:\n",
    "            validation_datasets = ['ptb', 'ptb-xl']\n",
    "            for file in os.listdir(dataset_path):\n",
    "                if file not in validation_datasets:\n",
    "                    file_path = os.path.join(dataset_path, file)\n",
    "                    file_path = file_path.replace('\\\\', '/')\n",
    "                    self.file_PATHS.append(file_path)\n",
    "\n",
    "        for path in self.file_PATHS:\n",
    "            if os.path.isdir(path):\n",
    "                for sub_folder in os.listdir(path):\n",
    "                    sub_folder_path = os.path.join(path, sub_folder)\n",
    "                    sub_folder_path = sub_folder_path.replace('\\\\', '/')\n",
    "                    \n",
    "                    # Ignore index.html files\n",
    "                    if sub_folder_path.endswith('index.html'):\n",
    "                        self._indices_files.append(sub_folder_path)\n",
    "                    else:\n",
    "                        if os.path.isdir(sub_folder_path):\n",
    "                            for file in os.listdir(sub_folder_path):\n",
    "                                # Get all .hea files\n",
    "                                if file.endswith('.hea'):\n",
    "                                    file_path = os.path.join(sub_folder_path, file)\n",
    "                                    file_path = file_path.replace('\\\\', '/')\n",
    "                                    self._hea_files.append(file_path)\n",
    "                                    self._hea_files_path.append(file_path)\n",
    "                                # Get all .mat files\n",
    "                                elif file.endswith('.mat'):\n",
    "                                    file_path = os.path.join(sub_folder_path, file)\n",
    "                                    file_path = file_path.replace('\\\\', '/')\n",
    "                                    self._mat_files.append(file_path)\n",
    "                                    self._mat_files_path.append(file_path)\n",
    "\n",
    "    def resample_ecg(self, data, old_freq, new_freq=128):\n",
    "        # Calculate the duration of the signal\n",
    "        duration = len(data) / old_freq\n",
    "\n",
    "        # Calculate the number of points in the resampled signal\n",
    "        num_points = int(np.round(duration * new_freq))\n",
    "\n",
    "        # Resample the signal\n",
    "        resampled_data = resample(data, num_points)\n",
    "\n",
    "        return resampled_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, slice):\n",
    "            start, stop, step = index.indices(len(self))\n",
    "            return [self[i] for i in range(start, stop, step)]\n",
    "        # 1. Get .hea file\n",
    "        hea_file_path = self._hea_files[index]\n",
    "        with open(hea_file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "        # Parse header information\n",
    "        # Initialize header information\n",
    "        header_info = {\n",
    "            'recording_number': lines[0].split()[0],\n",
    "            'recording_file': lines[0].split()[0] + '.mat',\n",
    "            'num_leads': int(lines[0].split()[1]),\n",
    "            'sampling_frequency': int(lines[0].split()[2]),\n",
    "            'num_samples': int(lines[0].split()[3]),\n",
    "            'leads_info': [],\n",
    "            'age': None,\n",
    "            'sex': None,\n",
    "            'dx': None,\n",
    "            'rx': None,\n",
    "            'hx': None,\n",
    "            'sx': None,\n",
    "        }\n",
    "\n",
    "        # Parse header information\n",
    "        for line in lines:\n",
    "            if line.startswith('# Age:'):\n",
    "                age_str = line.split(':')[1].strip()\n",
    "                header_info['age'] = int(age_str) if age_str != 'NaN' else None\n",
    "            elif line.startswith('# Sex:'):\n",
    "                header_info['sex'] = line.split(':')[1].strip()\n",
    "            elif line.startswith('# Dx:'):\n",
    "                header_info['dx'] = line.split(':')[1].strip().split(',')\n",
    "            elif line.startswith('# Rx:'):\n",
    "                header_info['rx'] = line.split(':')[1].strip()\n",
    "            elif line.startswith('# Hx:'):\n",
    "                header_info['hx'] = line.split(':')[1].strip()\n",
    "            elif line.startswith('# Sx:'):\n",
    "                header_info['sx'] = line.split(':')[1].strip()\n",
    "\n",
    "        for line in lines[1:header_info['num_leads']+1]:\n",
    "            adc_gain = line.split()[2].split('/')[0]\n",
    "            adc_gain = float(adc_gain.replace('(0)', ''))  # Remove '(0)' and convert to float\n",
    "            lead_info = {\n",
    "                'file': line.split()[0],\n",
    "                'adc_gain': adc_gain,\n",
    "                'units': line.split()[2].split('/')[1],\n",
    "                'adc_resolution': int(line.split()[3]),\n",
    "                'adc_zero': int(line.split()[4]),\n",
    "                'initial_value': int(line.split()[5]),\n",
    "                'checksum': int(line.split()[6]),\n",
    "                'lead_name': line.split()[7],\n",
    "            }\n",
    "            header_info['leads_info'].append(lead_info)\n",
    "\n",
    "        # 2. Get .mat file\n",
    "        twelve_lead_ecg = None\n",
    "        if index < len(self._mat_files):\n",
    "            mat_file_path = self._mat_files[index]\n",
    "            twelve_lead_ecg = sio.loadmat(mat_file_path)\n",
    "            \n",
    "            # Resample the ECG to 128 Hz\n",
    "            for lead in twelve_lead_ecg:\n",
    "                twelve_lead_ecg[lead] = self.resample_ecg(twelve_lead_ecg[lead], old_freq=header_info['sampling_frequency'])\n",
    "        else:\n",
    "            print(f\"MAT file for index {index} does not exist.\")\n",
    "        \n",
    "        return header_info, twelve_lead_ecg\n",
    "\n",
    "    def plot_record(self, index):\n",
    "        mat_file_path = self._mat_files[index]\n",
    "        data = sio.loadmat(mat_file_path)\n",
    "        fig, axs = plt.subplots(nrows=3, ncols=4, figsize=(20, 15))\n",
    "\n",
    "        for i, ax in enumerate(axs.flat):\n",
    "            ax.plot(data['val'][i], linewidth=0.5)\n",
    "            ax.set_xlabel('Sample')\n",
    "            ax.set_ylabel('Amplitude')\n",
    "            ax.set_title(f'Lead {i+1}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._hea_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PhysioNetDataset(PhysioNet_PATH, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_set = train_set[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recording_number': 'JS00104',\n",
       " 'recording_file': 'JS00104.mat',\n",
       " 'num_leads': 12,\n",
       " 'sampling_frequency': 500,\n",
       " 'num_samples': 5000,\n",
       " 'leads_info': [{'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -142,\n",
       "   'checksum': 16634,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 39,\n",
       "   'checksum': 12464,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 181,\n",
       "   'checksum': -4171,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 54,\n",
       "   'checksum': -8466,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -161,\n",
       "   'checksum': 4236,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 107,\n",
       "   'checksum': -1955,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 93,\n",
       "   'checksum': -5919,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 73,\n",
       "   'checksum': 18922,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 68,\n",
       "   'checksum': 29740,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 63,\n",
       "   'checksum': 29990,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 44,\n",
       "   'checksum': 27420,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00104.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 44,\n",
       "   'checksum': 27373,\n",
       "   'lead_name': '0'}],\n",
       " 'age': 68,\n",
       " 'sex': 'Male',\n",
       " 'dx': ['426177001'],\n",
       " 'rx': 'Unknown',\n",
       " 'hx': 'Unknown',\n",
       " 'sx': 'Unknown'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_train_set[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recording_number                       JS00104\n",
       "recording_file                     JS00104.mat\n",
       "num_leads                                   12\n",
       "sampling_frequency                         500\n",
       "num_samples                               5000\n",
       "                                 ...          \n",
       "lead_11_adc_zero                             0\n",
       "lead_11_initial_value                       44\n",
       "lead_11_checksum                         27373\n",
       "lead_11_lead_name                            0\n",
       "dx_modality              ['sinus bradycardia']\n",
       "Name: 99, Length: 108, dtype: object"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_df.iloc[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, criterion, and optimizer\n",
    "model = CLIPModel(train_set, processed_train_df)\n",
    "optimizer = torch.optim.Adam(model.ecg_encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = CLIPModel(train_set, processed_train_df)\n",
    "optimizer = torch.optim.Adam(model.ecg_encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/2:   0%|          | 0/65900 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n",
      "torch.Size([1, 79872])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1/2:   0%|          | 0/65900 [00:15<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 79872])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\navme\\Desktop\\ECG_Project\\Notebooks\\PhysioNet2021(Current Ver).ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m ecgs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(ecgs)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m model(ecgs, diagnoses)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\navme\\Desktop\\ECG_Project\\Notebooks\\PhysioNet2021(Current Ver).ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ecgs_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mecg_encoder(ecgs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m diagnoses_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_encoder\u001b[39m.\u001b[39mencode(diagnoses)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m positive_instances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minstance_selector\u001b[39m.\u001b[39;49mget_positive_instances()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m negative_instances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minstance_selector\u001b[39m.\u001b[39mget_negative_instances()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Compute loss based on whether the pair of embeddings is a positive or negative instance\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\navme\\Desktop\\ECG_Project\\Notebooks\\PhysioNet2021(Current Ver).ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_set)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     ecg_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mecg_encoder\u001b[39m.\u001b[39mencode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_set[i][\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     dx_modality_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtext_encoder\u001b[39m.\u001b[39;49mencode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessed_train_df[\u001b[39m'\u001b[39;49m\u001b[39mdx_modality\u001b[39;49m\u001b[39m'\u001b[39;49m][i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mall(torch\u001b[39m.\u001b[39meq(ecg_embedding, dx_modality_embedding)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         positive_instances\u001b[39m.\u001b[39mappend((ecg_embedding, dx_modality_embedding))\n",
      "\u001b[1;32mc:\\Users\\navme\\Desktop\\ECG_Project\\Notebooks\\PhysioNet2021(Current Ver).ipynb Cell 56\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Get embeddings from ClinicalBERT model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Average the embeddings to get single vector per each input\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/PhysioNet2021%28Current%20Ver%29.ipynb#Y112sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(embeddings, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    613\u001b[0m         hidden_states,\n\u001b[0;32m    614\u001b[0m         attention_mask,\n\u001b[0;32m    615\u001b[0m         layer_head_mask,\n\u001b[0;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    618\u001b[0m         past_key_value,\n\u001b[0;32m    619\u001b[0m         output_attentions,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:349\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    346\u001b[0m         relative_position_scores_key \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbhrd,lrd->bhlr\u001b[39m\u001b[39m\"\u001b[39m, key_layer, positional_embedding)\n\u001b[0;32m    347\u001b[0m         attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m relative_position_scores_query \u001b[39m+\u001b[39m relative_position_scores_key\n\u001b[1;32m--> 349\u001b[0m attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49msqrt(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_head_size)\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    351\u001b[0m     \u001b[39m# Apply the attention mask is (precomputed for all layers in BertModel forward() function)\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     attention_scores \u001b[39m=\u001b[39m attention_scores \u001b[39m+\u001b[39m attention_mask\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize a list to store the loss at each step\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(2):\n",
    "    # Add a progress bar for the inner loop\n",
    "    for i in tqdm(range(len(train_set)), desc=f\"Training epoch {epoch+1}/{2}\"):\n",
    "        # Get ECGs and diagnoses from training set\n",
    "        ecgs = train_set[i][1]['val']\n",
    "        diagnoses = processed_train_df['dx_modality'][i]\n",
    "\n",
    "        # Convert ECGs to tensor and add a dimension for batch size\n",
    "        ecgs = torch.from_numpy(ecgs).float().unsqueeze(0)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = model(ecgs, diagnoses)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save the loss to a variable\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(train_set)):\n",
    "        # Get ECG signal and dx_modality from training set\n",
    "        ecg_signal = train_set[i][1]['val']\n",
    "        dx_modality = processed_train_df['dx_modality'][i]\n",
    "        # Get target label from instance selector\n",
    "        target = 1 if (ecg_signal, dx_modality) in model.instance_selector.get_positive_instances() else 0\n",
    "        target = torch.tensor([target], dtype=torch.float)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(ecg_signal, dx_modality)\n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
