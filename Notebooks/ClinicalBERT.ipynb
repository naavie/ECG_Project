{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from scipy.signal import resample\n",
    "\n",
    "# Matlab/WFDB files\n",
    "import scipy.io as sio\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/navme/Desktop/ECG_Project/PyFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *\n",
    "from dataset import PhysioNetDataset\n",
    "from tripletloss import TripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhysioNet_PATH = f'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'\n",
    "PhysioNet_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PhysioNetDataset(PhysioNet_PATH, train = True)\n",
    "val_set = PhysioNetDataset(PhysioNet_PATH, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_csv = pd.read_csv('processed_train_set_records.csv')\n",
    "processed_val_csv = pd.read_csv('processed_val_set_records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_csv.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "    def forward(self, ecg_signal):\n",
    "        # Convert ECG signal to string as BERT takes text as input\n",
    "        ecg_signal = ' '.join(map(str, ecg_signal))\n",
    "        inputs = self.tokenizer(ecg_signal, return_tensors=\"pt\")\n",
    "        outputs = self.bert(**inputs)\n",
    "        # Use the BERT embeddings for the [CLS] token (first token)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ECGEncoder\n",
    "ecg_encoder = ECGEncoder()\n",
    "\n",
    "# Assume we have an ECG signal from the PhysioNetDataset\n",
    "_, ecg_signal = train_set[0]  # Get the first sample from the training set\n",
    "\n",
    "# We'll just use one of the leads for this example\n",
    "lead_name = list(ecg_signal.keys())[0]  # Get the name of the first lead\n",
    "lead_ecg_signal = ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "\n",
    "# Convert the ECG signal to a ClinicalBERT embedding\n",
    "ecg_embedding = ecg_encoder(lead_ecg_signal)\n",
    "\n",
    "# Now ecg_embedding is a tensor containing the ClinicalBERT embedding of the ECG signal\n",
    "print(ecg_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(CLIPModel, self).__init__()\n",
    "        self.ecg_encoder = ECGEncoder()\n",
    "        self.triplet_loss = TripletLoss(margin)\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        anchor_embedding = self.ecg_encoder(anchor)\n",
    "        positive_embedding = self.ecg_encoder(positive)\n",
    "        negative_embedding = self.ecg_encoder(negative)\n",
    "\n",
    "        loss = self.triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CLIPModel\n",
    "clip_model = CLIPModel()\n",
    "\n",
    "# Assume we have an anchor, positive, and negative ECG signals from the PhysioNetDataset\n",
    "_, anchor_ecg_signal = train_set[0]  # Get the first sample from the training set\n",
    "_, positive_ecg_signal = train_set[1]  # Get the second sample from the training set\n",
    "_, negative_ecg_signal = train_set[2]  # Get the third sample from the training set\n",
    "\n",
    "# We'll just use one of the leads for this example\n",
    "lead_name = list(anchor_ecg_signal.keys())[0]  # Get the name of the first lead\n",
    "anchor_lead_ecg_signal = anchor_ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "positive_lead_ecg_signal = positive_ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "negative_lead_ecg_signal = negative_ecg_signal[lead_name]  # Get the ECG signal for that lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0096, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the triplet loss for these ECG signals\n",
    "triplet_loss = clip_model(anchor_lead_ecg_signal, positive_lead_ecg_signal, negative_lead_ecg_signal)\n",
    "\n",
    "# Now triplet_loss is a tensor containing the triplet loss for these ECG signals\n",
    "print(triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Instantiate the CLIPModel\n",
    "clip_model = CLIPModel()\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(clip_model.parameters())\n",
    "\n",
    "# Loop over your training data\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(train_set)):\n",
    "        # Get the anchor, positive, and negative ECG signals\n",
    "        _, anchor_ecg_signal = train_set[i]\n",
    "        _, positive_ecg_signal = train_set[(i + 1) % len(train_set)]  # Use the next sample as the positive example\n",
    "        _, negative_ecg_signal = train_set[(i + 2) % len(train_set)]  # Use the sample after that as the negative example\n",
    "\n",
    "        # We'll just use one of the leads for this example\n",
    "        lead_name = list(anchor_ecg_signal.keys())[0]  # Get the name of the first lead\n",
    "        anchor_lead_ecg_signal = anchor_ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "        positive_lead_ecg_signal = positive_ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "        negative_lead_ecg_signal = negative_ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "\n",
    "        # Compute the triplet loss for these ECG signals\n",
    "        triplet_loss = clip_model(anchor_lead_ecg_signal, positive_lead_ecg_signal, negative_lead_ecg_signal)\n",
    "\n",
    "        # Backpropagate the loss and update the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {triplet_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The `ECGEncoder` is used to convert the raw ECG signals into embeddings using the ClinicalBERT model.\n",
    "\n",
    "2. The `CLIPModel` uses the `ECGEncoder` to get embeddings for the anchor, positive, and negative examples, and then computes the triplet loss on these embeddings. By training the `CLIPModel` on your dataset, you're effectively learning a new representation for your ECG signals where similar signals are close together and different signals are far apart.\n",
    "\n",
    "3. Once you've trained the `CLIPModel`, you can use it to transform your ECG signals into this new representation. You can then train a classifier on these transformed signals. This classifier could be any type of model you choose, such as a linear classifier, a support vector machine, a decision tree, a neural network, etc. The hope is that the new representation learned by the `CLIPModel` will be more useful for classification than the raw ECG signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the data\n",
    "train_dataset = PhysioNetDataset(processed_train_csv)\n",
    "val_dataset = PhysioNetDataset(processed_val_csv)\n",
    "\n",
    "# Instantiate the CLIPModel\n",
    "clip_model = CLIPModel()\n",
    "\n",
    "# Define the 1D CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(64, num_classes)  # num_classes is the number of unique values in 'dx_modality'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "cnn = CNN()\n",
    "\n",
    "# Define a loss function and an optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters())\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(train_dataset)):\n",
    "        # Get the ECG signal and the label\n",
    "        _, ecg_signal = train_dataset[i]\n",
    "        label = processed_train_csv['dx_modality'][i]\n",
    "\n",
    "        # Transform the ECG signal using the CLIPModel\n",
    "        ecg_signal = clip_model(ecg_signal)\n",
    "\n",
    "        # Train the CNN on the transformed ECG signal and the label\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(ecg_signal)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(val_dataset)):\n",
    "        # Get the ECG signal and the label\n",
    "        _, ecg_signal = val_dataset[i]\n",
    "        label = processed_val_csv['dx_modality'][i]\n",
    "\n",
    "        # Transform the ECG signal using the CLIPModel\n",
    "        ecg_signal = clip_model(ecg_signal)\n",
    "\n",
    "        # Compute the model's prediction\n",
    "        outputs = cnn(ecg_signal)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the validation set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the class of a new ECG signal using the trained 1D CNN model, you can follow these steps:\n",
    "\n",
    "1. Load the new ECG signal.\n",
    "2. Transform the ECG signal into the learned representation using the `CLIPModel`.\n",
    "3. Pass the transformed ECG signal through the 1D CNN to get the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "# Load the new ECG signal\n",
    "new_ecg_signal = loadmat('new_ecg_signal.mat')\n",
    "\n",
    "# Assuming the ECG signal is stored in a variable named 'ecg' in the .mat file\n",
    "new_ecg_signal = new_ecg_signal['ecg']\n",
    "\n",
    "# Transform the ECG signal using the CLIPModel\n",
    "new_ecg_signal = clip_model(new_ecg_signal)\n",
    "\n",
    "# Pass the transformed ECG signal through the 1D CNN to get the predicted class\n",
    "outputs = cnn(new_ecg_signal)\n",
    "_, predicted_class = torch.max(outputs.data, 1)\n",
    "\n",
    "print(f\"The predicted class for the new ECG signal is: {predicted_class.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads a new ECG signal, transforms it using the `CLIPModel`, and then passes the transformed signal through the 1D CNN to get the predicted class. The predicted class is then printed to the console. Note that you'll need to replace `'new_ecg_signal.mat'` with the path to your new ECG signal file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
