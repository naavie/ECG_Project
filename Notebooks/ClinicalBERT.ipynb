{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClinicalBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from scipy.signal import resample\n",
    "\n",
    "# Matlab/WFDB files\n",
    "import scipy.io as sio\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/navme/Desktop/ECG_Project/PyFiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import *\n",
    "from dataset import PhysioNetDataset\n",
    "from tripletloss import TripletLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PhysioNet_PATH = f'C:/Users/navme/Desktop/ECG_Thesis_Local/PhysioNet-2021-Challenge/physionet.org/files/challenge-2021/1.0.3/training'\n",
    "PhysioNet_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = PhysioNetDataset(PhysioNet_PATH, train = True)\n",
    "val_set = PhysioNetDataset(PhysioNet_PATH, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_csv = pd.read_csv('processed_train_set_records.csv')\n",
    "processed_val_csv = pd.read_csv('processed_val_set_records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['recording_number', 'recording_file', 'num_leads',\n",
       "       'sampling_frequency', 'num_samples', 'age', 'sex', 'dx', 'rx',\n",
       "       'hx', 'sx', 'lead_0_file', 'lead_0_adc_gain', 'lead_0_units',\n",
       "       'lead_0_adc_resolution', 'lead_0_adc_zero', 'lead_0_initial_value',\n",
       "       'lead_0_checksum', 'lead_0_lead_name', 'lead_1_file',\n",
       "       'lead_1_adc_gain', 'lead_1_units', 'lead_1_adc_resolution',\n",
       "       'lead_1_adc_zero', 'lead_1_initial_value', 'lead_1_checksum',\n",
       "       'lead_1_lead_name', 'lead_2_file', 'lead_2_adc_gain',\n",
       "       'lead_2_units', 'lead_2_adc_resolution', 'lead_2_adc_zero',\n",
       "       'lead_2_initial_value', 'lead_2_checksum', 'lead_2_lead_name',\n",
       "       'lead_3_file', 'lead_3_adc_gain', 'lead_3_units',\n",
       "       'lead_3_adc_resolution', 'lead_3_adc_zero', 'lead_3_initial_value',\n",
       "       'lead_3_checksum', 'lead_3_lead_name', 'lead_4_file',\n",
       "       'lead_4_adc_gain', 'lead_4_units', 'lead_4_adc_resolution',\n",
       "       'lead_4_adc_zero', 'lead_4_initial_value', 'lead_4_checksum',\n",
       "       'lead_4_lead_name', 'lead_5_file', 'lead_5_adc_gain',\n",
       "       'lead_5_units', 'lead_5_adc_resolution', 'lead_5_adc_zero',\n",
       "       'lead_5_initial_value', 'lead_5_checksum', 'lead_5_lead_name',\n",
       "       'lead_6_file', 'lead_6_adc_gain', 'lead_6_units',\n",
       "       'lead_6_adc_resolution', 'lead_6_adc_zero', 'lead_6_initial_value',\n",
       "       'lead_6_checksum', 'lead_6_lead_name', 'lead_7_file',\n",
       "       'lead_7_adc_gain', 'lead_7_units', 'lead_7_adc_resolution',\n",
       "       'lead_7_adc_zero', 'lead_7_initial_value', 'lead_7_checksum',\n",
       "       'lead_7_lead_name', 'lead_8_file', 'lead_8_adc_gain',\n",
       "       'lead_8_units', 'lead_8_adc_resolution', 'lead_8_adc_zero',\n",
       "       'lead_8_initial_value', 'lead_8_checksum', 'lead_8_lead_name',\n",
       "       'lead_9_file', 'lead_9_adc_gain', 'lead_9_units',\n",
       "       'lead_9_adc_resolution', 'lead_9_adc_zero', 'lead_9_initial_value',\n",
       "       'lead_9_checksum', 'lead_9_lead_name', 'lead_10_file',\n",
       "       'lead_10_adc_gain', 'lead_10_units', 'lead_10_adc_resolution',\n",
       "       'lead_10_adc_zero', 'lead_10_initial_value', 'lead_10_checksum',\n",
       "       'lead_10_lead_name', 'lead_11_file', 'lead_11_adc_gain',\n",
       "       'lead_11_units', 'lead_11_adc_resolution', 'lead_11_adc_zero',\n",
       "       'lead_11_initial_value', 'lead_11_checksum', 'lead_11_lead_name',\n",
       "       'dx_modality'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_csv.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': array([[408.24601882, 408.24601882, 408.24601882, ..., -83.34581329,\n",
       "         -74.965045  , -63.10339951],\n",
       "        [-92.07603073, -92.07603073, -92.07603073, ...,  57.20010276,\n",
       "          54.51591647,  58.88514819],\n",
       "        [225.08001192, 225.08001192, 225.08001192, ...,  93.39571052,\n",
       "          97.44912853, 117.96825132]])}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "    def forward(self, ecg_signal):\n",
    "        # Convert ECG signal to string as BERT takes text as input\n",
    "        ecg_signal = ' '.join(map(str, ecg_signal))\n",
    "        inputs = self.tokenizer(ecg_signal, return_tensors=\"pt\")\n",
    "        outputs = self.bert(**inputs)\n",
    "        # Use the BERT embeddings for the [CLS] token (first token)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECGEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ECGEncoder, self).__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        \n",
    "    def forward(self, data):\n",
    "        metadata, ecg_signal = data\n",
    "        embeddings = []\n",
    "        for lead_info, lead_signal in zip(metadata['leads_info'], ecg_signal):\n",
    "            # Create a textual description of the ECG signal\n",
    "            description = f\"Lead {lead_info['lead_name']} with initial value {lead_info['initial_value']} and checksum {lead_info['checksum']} has signal values {lead_signal}\"\n",
    "            inputs = self.tokenizer(description, return_tensors=\"pt\")\n",
    "            outputs = self.bert(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state[:, 0, :])\n",
    "        return torch.cat(embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for lead 0: tensor([ 2.6361e-01,  8.8632e-02, -4.2674e-01,  3.2769e-01,  2.7432e-01,\n",
      "        -7.9961e-02,  4.5787e-01,  2.4446e-01,  7.8162e-02, -2.5685e-01,\n",
      "        -4.1844e-01, -6.6938e-02, -4.2574e-01,  3.7564e-01, -3.5149e-01,\n",
      "         1.3120e-01,  3.5058e-01,  2.5360e-01, -4.3578e-01,  1.7420e-01,\n",
      "         9.3913e-02,  1.9671e-01, -1.7270e-01, -2.4773e-01, -3.5569e-01,\n",
      "        -5.1214e-01,  9.7906e-01,  4.0506e-01,  2.7232e-02,  4.5981e-01,\n",
      "         5.2280e-01, -1.4626e-03, -9.2910e-03, -1.2541e-01, -1.5461e-01,\n",
      "        -5.4510e-02,  1.0910e-01,  2.1561e-01, -9.0366e-02, -6.8356e-02,\n",
      "        -3.3390e-03,  3.2699e-02,  1.0143e+00, -4.8333e-02,  3.7887e-01,\n",
      "        -4.9696e-01, -2.7866e-01,  6.3696e-01, -4.6769e-01,  2.4993e-01,\n",
      "         1.5906e-02,  5.8978e-01, -2.1196e-03, -7.6264e-02,  9.2241e-02,\n",
      "        -1.0423e-01, -1.0745e-01, -4.0016e-01, -9.6476e-02, -6.6540e-02,\n",
      "         2.5644e-01, -1.0989e-02,  3.5057e-01, -5.9316e-02, -1.7401e-01,\n",
      "        -2.0260e-01, -1.2896e-01,  2.9800e-01, -5.6981e-02, -3.9799e-03,\n",
      "        -4.3721e-01, -5.1358e-01, -2.2981e-01,  3.5621e-01,  1.8034e-01,\n",
      "        -1.3571e-01,  2.1111e-01, -5.3724e-01,  5.6061e-01,  2.1006e-01,\n",
      "         5.1270e-02, -3.2814e-01, -8.2725e-02,  4.7858e-01, -8.2194e-01,\n",
      "         2.2285e-01, -1.1733e-01, -2.4871e-01, -4.4166e-01,  3.1294e-01,\n",
      "         6.0392e-01, -7.9006e-01, -2.6261e-01,  1.8362e-01, -6.3389e-01,\n",
      "         4.4172e-01, -3.5788e-03,  5.0622e-02,  6.0104e-01, -1.2608e-01,\n",
      "         2.4187e-01,  1.8464e-01, -5.7598e-02,  1.2563e-01,  4.5295e-01,\n",
      "        -2.5716e-01,  3.5399e-01, -5.1853e-01,  5.7668e-02,  3.4476e-01,\n",
      "        -4.6298e-02, -3.4248e-01, -2.6983e-01,  1.5614e-01,  4.4215e-02,\n",
      "        -1.0338e-01,  7.3049e-02,  2.1905e-01,  2.5314e-01, -3.5637e-01,\n",
      "         3.0749e-01, -1.0251e-01,  1.6604e-01,  4.0702e-01,  5.5540e-02,\n",
      "        -3.3024e-01,  2.9034e-01,  1.2032e-03,  2.0660e-01, -2.2486e-01,\n",
      "        -5.6604e-01, -3.3707e-01, -2.1823e-01,  2.1625e-01,  5.4786e-04,\n",
      "         4.4512e-01,  2.0753e-01, -1.3294e-01, -1.0687e+00,  2.5125e-01,\n",
      "        -4.2237e-01, -3.9042e-01,  1.2649e-01, -4.0321e-01, -3.3560e-01,\n",
      "        -3.3186e-02,  5.6052e-02, -1.7406e-01, -4.6450e-01,  1.1297e-01,\n",
      "         7.7756e-01, -4.7337e-01, -5.7637e-02,  6.5204e-01, -3.6011e-01,\n",
      "        -4.1682e-01, -2.2176e-01,  1.6461e-01, -1.3588e-01, -4.8693e-01,\n",
      "         9.5880e-02,  3.7846e-01,  2.1775e-01,  2.9463e-01,  9.6811e-02,\n",
      "        -2.1066e-01, -2.6307e-01, -1.2595e-04,  6.8805e-02,  2.9264e-02,\n",
      "        -3.2586e-01,  9.8059e-02, -1.7734e-01,  5.5056e-01,  1.2292e-01,\n",
      "         7.7534e-01, -5.8070e-01, -5.1529e-01, -4.1863e-01,  4.4975e-02,\n",
      "        -2.0217e-01,  5.9917e-02,  5.0169e-01,  2.5564e-01, -4.7027e-01,\n",
      "         1.7300e-01,  5.7924e-01, -3.8829e-01,  6.7269e-02,  3.8517e-01,\n",
      "        -1.5008e-01, -2.2588e-01,  3.0290e-01,  9.6121e-02, -7.1724e-02,\n",
      "        -1.4961e-01, -4.6312e-01, -1.4714e-01, -2.5680e-02, -4.2295e-01,\n",
      "         1.3652e-01,  1.5827e-01, -4.5612e-01,  1.0005e-01,  3.7225e-01,\n",
      "         3.0194e-02,  4.6093e-01,  3.6438e-01,  2.6146e-01,  6.1027e-02,\n",
      "        -1.0262e-01, -2.8081e-01,  8.4455e-02,  1.7438e-02,  1.8294e-01,\n",
      "        -3.7618e-01,  4.3277e-01,  1.6551e-03, -6.5862e-01,  1.0947e-01,\n",
      "         3.6958e-01, -5.2447e-01,  1.2547e-01,  1.1857e-01, -7.5209e-01,\n",
      "         3.6306e-01,  4.9482e-01,  7.7738e-02, -5.0323e-02, -2.6844e-01,\n",
      "        -3.1657e-01, -7.0228e-03,  1.7167e-01, -3.2948e-02, -1.0383e-01,\n",
      "        -7.5062e-02,  6.7149e-02,  8.9236e-02, -1.4971e-01,  3.9476e-01,\n",
      "         3.1403e-01,  1.0273e-01,  2.7723e-01,  2.8027e-02, -2.7049e-01,\n",
      "        -4.2576e-02,  2.3004e-01, -1.5490e-02, -2.7879e-01,  2.6302e-01,\n",
      "         3.8506e-01, -1.1550e-01, -2.1171e-01, -3.7582e-01, -1.7786e-01,\n",
      "        -3.5635e-01, -1.2985e-01, -4.4411e-02, -1.0298e+00, -1.8277e-01,\n",
      "         5.5159e-01, -4.0201e-01,  5.5002e-02, -4.0472e-01,  1.9975e-01,\n",
      "         6.3785e-02,  1.6841e-01,  3.4047e-01, -3.8845e-01,  3.5392e-02,\n",
      "        -4.2617e-02, -5.2161e-01, -1.5921e-01,  3.3723e-02, -2.7164e-01,\n",
      "         4.8277e-01, -3.6664e-01,  9.6992e-02,  2.3893e-01, -1.6750e-01,\n",
      "         1.4067e-01,  2.7775e-01, -4.3318e-01,  1.2644e-01, -1.5176e-01,\n",
      "         4.7064e-01,  2.9814e-01,  4.5002e-02,  2.8555e-01,  5.0397e-01,\n",
      "         6.3900e-01,  2.0235e-01, -1.0892e-01, -2.2246e-01, -1.7920e-02,\n",
      "         1.3556e-01, -1.7700e-01,  5.6967e-01, -2.0784e-01,  9.3014e-02,\n",
      "        -5.1680e-02,  4.8257e-01, -1.7803e-01, -1.7745e-01, -2.4198e-01,\n",
      "         5.2734e-01, -4.2273e-01,  1.2908e-01, -8.7125e-02,  2.3239e-01,\n",
      "         1.2987e-01,  3.5013e-01, -1.4133e-01,  2.2241e-01, -1.3673e-01,\n",
      "        -4.4830e-01,  2.2214e-01, -1.2564e-01,  5.2724e-01,  2.9385e-01,\n",
      "        -1.1846e-01, -9.7237e-02, -1.9567e-01, -5.1743e-01,  3.8173e-02,\n",
      "        -3.1987e-01, -3.3995e-01, -3.6679e-01,  3.1993e-01,  2.4973e-01,\n",
      "         2.6328e-01,  5.3625e-01, -1.8457e-01, -1.6823e-01,  1.7425e-01,\n",
      "        -5.3570e-01, -2.8482e-01,  3.9608e-01, -4.0630e-01,  3.2640e-01,\n",
      "         5.3932e-02, -4.2785e-01,  2.7386e-02,  2.2464e-01,  4.3917e-01,\n",
      "        -1.5079e-01,  1.2886e-01, -5.4915e-01,  3.3698e-01,  4.6220e-02,\n",
      "         3.4145e-01,  2.0384e-01, -3.1539e-01,  6.1164e-01, -4.1782e-02,\n",
      "        -4.9112e-01,  7.4283e-02, -1.2537e-01,  1.1292e-01,  3.8961e-02,\n",
      "         2.5372e-01, -8.4581e-02, -3.3477e-02,  6.4001e-01, -9.5819e-01,\n",
      "        -2.4303e-03,  6.5107e-02, -1.0583e-01, -2.8094e-01, -5.8490e-02,\n",
      "         4.5263e-01, -6.2568e-02,  4.7923e-01,  1.1545e-01, -1.9792e-01,\n",
      "         2.9781e-01, -7.9465e-02,  3.0239e-01, -1.1577e-02, -3.1118e-01,\n",
      "        -2.3698e-01,  5.4051e-01,  1.6661e-02, -2.8741e-01, -2.3311e-01,\n",
      "        -1.6950e-01, -1.6550e-01, -4.0061e-01,  9.3901e-02, -4.1733e-02,\n",
      "         1.2206e-01, -1.2754e-01, -5.0213e-01, -1.2095e-01, -2.6450e-01,\n",
      "         5.7318e-01,  3.9357e-01,  3.0070e-02,  3.2833e-02, -2.3737e-01,\n",
      "        -2.5904e-01,  4.5721e-01,  2.5958e-01,  2.4734e-01,  2.4318e-01,\n",
      "         1.9454e-01,  9.8280e-02, -5.6695e-01, -3.6042e-02,  1.9527e-01,\n",
      "        -2.7238e-01, -1.1036e-01, -8.1059e-02, -4.4244e-01, -3.2450e-01,\n",
      "         6.9577e-02, -2.5404e-01,  4.2114e-01,  1.3494e-01, -3.8652e-02,\n",
      "         8.2434e-02,  4.4606e-02,  3.0203e-01, -7.2417e-01,  3.5067e-01,\n",
      "         3.8935e-01, -1.9115e-01,  3.5466e-02, -6.6439e-02, -9.0320e-02,\n",
      "         1.6659e-01,  4.1250e-02,  2.3817e-01,  1.0087e-01, -4.4026e-01,\n",
      "        -5.3688e-02,  2.4878e-01, -4.7529e-01,  1.5193e-01,  7.9057e-01,\n",
      "        -2.9931e-01,  6.4267e-01, -7.3614e-03, -3.3202e-01,  1.2577e-01,\n",
      "        -3.3254e-01, -2.3936e-01,  8.6173e-02,  2.0195e-01,  1.4205e-01,\n",
      "        -3.3958e-01,  2.0051e-01, -4.4952e-03, -1.9665e-01,  2.6315e-01,\n",
      "         4.0677e-02,  5.1205e-01, -3.8169e-02,  1.3808e-02, -5.6980e-02,\n",
      "        -2.4606e-01, -4.6877e-01, -1.2989e-01,  1.0529e-01,  2.4662e-01,\n",
      "         1.4764e-01,  3.7309e-01, -1.8173e-01, -3.0616e-01,  2.9131e-01,\n",
      "        -2.5381e-01, -3.3087e-01,  4.3262e-01,  1.7157e-01, -1.2950e-01,\n",
      "         1.3908e-01, -5.2069e-02,  2.7660e-01, -1.4495e-01,  3.8585e-01,\n",
      "         3.6071e-01,  3.9689e-01, -7.0421e-02,  5.4372e-01,  5.3883e-01,\n",
      "        -2.6728e-01,  1.1969e-01, -4.5937e-01, -9.1697e-02, -2.2315e-01,\n",
      "        -1.0483e-01,  5.6460e-01, -3.5765e-01, -2.4640e-01,  3.8643e-01,\n",
      "         8.6542e-02, -1.5933e-01, -2.3271e-01, -4.1047e-02, -4.3910e-02,\n",
      "         3.2751e-01, -1.2291e-01, -5.6581e-02, -4.5016e-01, -1.1961e-01,\n",
      "        -1.5067e-01,  2.0457e-01,  3.5473e-01,  4.2337e-01, -3.6405e-01,\n",
      "         8.9583e-02, -4.5130e-01,  1.5865e-01,  1.4476e-01,  6.4407e-02,\n",
      "        -1.1199e-01, -3.2198e-01,  1.7836e-01,  4.8173e-01, -2.8375e-01,\n",
      "         2.9432e-01,  1.6656e-01, -5.2647e-01,  3.1991e-01, -4.7275e-01,\n",
      "        -2.1081e-01,  1.4262e-01, -1.9390e-01,  1.2222e-01, -4.1305e-01,\n",
      "         2.4535e-01,  5.6410e-02,  2.0454e-01, -1.1453e-01, -1.9265e-01,\n",
      "         1.4990e-01, -3.3686e-01, -2.8021e-01, -1.2508e-01,  5.7227e-01,\n",
      "         3.6769e-01, -4.7703e-01,  3.0810e-01, -2.7139e-01,  2.0384e-01,\n",
      "         1.9460e-01, -9.3608e-02, -4.2176e-01,  1.1109e-01,  2.9634e-01,\n",
      "         5.9264e-01,  4.5815e-01, -6.1271e-01,  5.8991e-01,  4.5789e-01,\n",
      "         4.8428e-02, -9.1488e-02, -1.0804e+01, -1.5017e-01, -2.6332e-01,\n",
      "         3.6909e-01, -3.3880e-01, -1.0500e-01,  7.0470e-01, -1.9132e-01,\n",
      "         1.6321e-01,  3.7837e-01, -3.0217e-01, -2.3571e-01, -2.0361e-01,\n",
      "         3.5063e-02,  2.4212e-01,  8.4239e-02, -8.9025e-02,  3.8934e-02,\n",
      "         2.7722e-01,  6.1696e-01,  4.5974e-02,  7.7532e-01,  1.0289e-01,\n",
      "         1.4298e-01, -2.0622e-01, -2.2570e-01, -4.3268e-01,  1.7199e-01,\n",
      "         2.6622e-02, -1.8999e-01,  1.8821e-01,  1.7669e-01, -2.1499e-03,\n",
      "         1.8502e-01,  9.6272e-02, -1.4118e-01, -1.8093e-01,  2.5345e-01,\n",
      "        -5.9624e-02, -1.6993e-01,  1.6681e-01, -5.5915e-01,  7.0434e-01,\n",
      "         2.0646e-01,  7.0710e-02, -3.4066e-01, -1.8807e-01, -1.7418e+00,\n",
      "         3.4384e-01,  4.5172e-01,  7.1287e-01,  2.3312e-02,  3.6026e-01,\n",
      "         2.5276e-01,  6.0225e-02, -1.0431e-01,  6.4627e-02,  6.5483e-02,\n",
      "        -2.0718e-01, -6.5420e-02,  1.5281e-02,  2.6067e-01,  8.0573e-01,\n",
      "         1.0558e-01, -2.5260e-01,  1.4775e-01, -6.6207e-02, -1.6576e-01,\n",
      "         4.3399e-02, -8.7605e-01, -3.2607e-01,  5.9358e-01, -2.1778e-01,\n",
      "        -4.2319e-02,  3.4129e-01,  1.3009e-01, -2.4777e-01,  3.3737e-01,\n",
      "         4.0150e-01,  4.2545e-01,  1.2786e-01, -9.5565e-02, -2.7977e-01,\n",
      "        -4.9568e-02,  2.1103e-01, -4.1065e-01,  1.5306e-01,  4.0284e-01,\n",
      "        -2.4215e-01,  1.0197e-01, -3.2703e-01,  4.7232e-02, -7.4362e-02,\n",
      "        -4.5349e-01, -4.1566e-01, -3.8584e-02, -3.9482e-01,  6.4507e-02,\n",
      "        -1.4417e-01, -6.7139e-01,  9.5781e-02,  4.2686e-01, -1.0161e-01,\n",
      "        -7.4094e-01, -4.3202e-01, -1.6491e-01,  7.7688e-01,  2.0017e-01,\n",
      "        -1.0872e-02,  1.9447e-01, -5.0493e-01, -3.4067e-01,  2.0426e-01,\n",
      "         6.5807e-01,  6.4978e-01,  6.1006e-02,  2.6950e-02, -8.0741e-01,\n",
      "        -2.1182e-01, -2.4331e-01,  3.2757e-01, -4.3078e-01,  1.1342e-02,\n",
      "         1.5541e-01, -6.4751e-02,  3.2637e-02,  7.7842e-01, -2.8301e-02,\n",
      "         1.4410e-01, -1.2112e-01,  3.1963e-01,  4.7741e-01, -2.8834e-01,\n",
      "         4.6342e-01,  2.1658e-01, -2.2374e-01,  3.5775e-01, -9.5673e-02,\n",
      "        -4.5092e-01,  3.2781e-01, -3.5951e-02,  2.5061e-01, -7.8705e-02,\n",
      "         7.7503e-01, -1.1067e-01, -9.9861e-02,  4.6874e-02,  1.3651e-01,\n",
      "        -3.1143e-01, -2.2225e-01, -3.1605e-02,  1.7398e-01, -1.8465e-01,\n",
      "        -7.5975e-02, -4.0751e-02,  1.5542e-01, -1.6225e-01, -3.8907e-01,\n",
      "        -3.4967e-01,  1.5501e-01,  2.0034e-01,  8.2910e-01, -4.8751e-02,\n",
      "        -1.0971e-01,  2.4434e-02, -2.0555e-01, -2.1398e-01,  2.1184e-01,\n",
      "        -5.8371e-02, -2.4323e-01, -7.5900e-01,  5.0913e-01,  3.3183e-01,\n",
      "        -5.0042e-01, -4.3887e-02, -9.7433e-02, -1.8907e-01, -3.1174e-01,\n",
      "        -1.8632e-01,  1.7622e-01,  1.3142e-01,  1.3073e-01,  7.3209e-02,\n",
      "        -7.3314e-01, -5.2736e-02,  3.6479e-01,  2.5554e-01, -4.2583e-03,\n",
      "         5.2229e-02, -3.5650e-02, -7.3840e-01,  1.9695e-02, -3.8333e-01,\n",
      "        -1.3942e-01,  4.7475e-01, -2.9943e-01, -3.8265e-01,  2.9240e-01,\n",
      "        -1.3944e-01,  5.2277e-01, -8.0391e-02,  2.7868e-01,  2.7106e-01,\n",
      "        -3.8777e-01,  3.6885e-01,  1.9122e-01,  1.6973e-01, -9.2847e-03,\n",
      "         6.3350e-02,  1.4724e-01, -2.2319e-02], grad_fn=<UnbindBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the ECGEncoder\n",
    "ecg_encoder = ECGEncoder()\n",
    "\n",
    "# Get the first data point from the training set\n",
    "data = train_set[0]\n",
    "\n",
    "# Generate a ClinicalBERT embedding for the data point\n",
    "embeddings = ecg_encoder(data)\n",
    "\n",
    "for i, embedding in enumerate(embeddings):\n",
    "    print(f\"Embedding for lead {i}: {embedding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ECGEncoder\n",
    "ecg_encoder = ECGEncoder()\n",
    "\n",
    "# Assume we have an ECG signal from the PhysioNetDataset\n",
    "_, ecg_signal = train_set[0]  # Get the first sample from the training set\n",
    "\n",
    "# We'll just use one of the leads for this example\n",
    "lead_name = list(ecg_signal.keys())[0]  # Get the name of the first lead\n",
    "lead_ecg_signal = ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "\n",
    "# Convert the ECG signal to a ClinicalBERT embedding\n",
    "ecg_embedding = ecg_encoder(lead_ecg_signal)\n",
    "\n",
    "# Now ecg_embedding is a tensor containing the ClinicalBERT embedding of the ECG signal\n",
    "print(ecg_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recording_number': 'JS00001',\n",
       " 'recording_file': 'JS00001.mat',\n",
       " 'num_leads': 12,\n",
       " 'sampling_frequency': 500,\n",
       " 'num_samples': 5000,\n",
       " 'leads_info': [{'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -254,\n",
       "   'checksum': 21756,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 264,\n",
       "   'checksum': -599,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 517,\n",
       "   'checksum': -22376,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -5,\n",
       "   'checksum': 28232,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -386,\n",
       "   'checksum': 16619,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 390,\n",
       "   'checksum': 15121,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -98,\n",
       "   'checksum': 1568,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -312,\n",
       "   'checksum': -32761,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': -98,\n",
       "   'checksum': 32715,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 810,\n",
       "   'checksum': 15193,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 810,\n",
       "   'checksum': 14081,\n",
       "   'lead_name': '0'},\n",
       "  {'file': 'JS00001.mat',\n",
       "   'adc_gain': 1000.0,\n",
       "   'units': 'mV',\n",
       "   'adc_resolution': 16,\n",
       "   'adc_zero': 0,\n",
       "   'initial_value': 527,\n",
       "   'checksum': 32579,\n",
       "   'lead_name': '0'}],\n",
       " 'age': 85,\n",
       " 'sex': 'Male',\n",
       " 'dx': ['164889003', '59118001', '164934002'],\n",
       " 'rx': 'Unknown',\n",
       " 'hx': 'Unknown',\n",
       " 'sx': 'Unknown'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(CLIPModel, self).__init__()\n",
    "        self.ecg_encoder = ECGEncoder()\n",
    "        self.triplet_loss = TripletLoss(margin)\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        anchor_embedding = self.ecg_encoder(anchor)\n",
    "        positive_embedding = self.ecg_encoder(positive)\n",
    "        negative_embedding = self.ecg_encoder(negative)\n",
    "\n",
    "        loss = self.triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'recording_number': 'JS00001', 'recording_file': 'JS00001.mat', 'num_leads': 12, 'sampling_frequency': 500, 'num_samples': 5000, 'leads_info': [{'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': -254, 'checksum': 21756, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': 264, 'checksum': -599, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': 517, 'checksum': -22376, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': -5, 'checksum': 28232, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': -386, 'checksum': 16619, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': 390, 'checksum': 15121, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': -98, 'checksum': 1568, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': -312, 'checksum': -32761, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': -98, 'checksum': 32715, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': 810, 'checksum': 15193, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': 810, 'checksum': 14081, 'lead_name': '0'}, {'file': 'JS00001.mat', 'adc_gain': 1000.0, 'units': 'mV', 'adc_resolution': 16, 'adc_zero': 0, 'initial_value': 527, 'checksum': 32579, 'lead_name': '0'}], 'age': 85, 'sex': 'Male', 'dx': ['164889003', '59118001', '164934002'], 'rx': 'Unknown', 'hx': 'Unknown', 'sx': 'Unknown'}, {'val': array([[408.24601882, 408.24601882, 408.24601882, ..., -83.34581329,\n",
      "        -74.965045  , -63.10339951],\n",
      "       [-92.07603073, -92.07603073, -92.07603073, ...,  57.20010276,\n",
      "         54.51591647,  58.88514819],\n",
      "       [225.08001192, 225.08001192, 225.08001192, ...,  93.39571052,\n",
      "         97.44912853, 117.96825132]])})\n"
     ]
    }
   ],
   "source": [
    "# Get the first data point from the training set\n",
    "data = train_set[0]\n",
    "\n",
    "# Print the data point to inspect its structure\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.2122842073440552\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = CLIPModel(margin=1.0)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Assume we have some data points\n",
    "anchor_metadata, anchor_data = train_set[0]\n",
    "positive_metadata, positive_data = train_set[1]\n",
    "negative_metadata, negative_data = train_set[2]\n",
    "\n",
    "# Extract the signal values\n",
    "anchor_signal = anchor_data['val']\n",
    "positive_signal = positive_data['val']\n",
    "negative_signal = negative_data['val']\n",
    "\n",
    "# Convert signal values to tensors\n",
    "anchor = torch.tensor(anchor_signal, dtype=torch.float32)\n",
    "positive = torch.tensor(positive_signal, dtype=torch.float32)\n",
    "negative = torch.tensor(negative_signal, dtype=torch.float32)\n",
    "\n",
    "# Zero the gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "loss = model((anchor_metadata, anchor), (positive_metadata, positive), (negative_metadata, negative))\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Update the weights\n",
    "optimizer.step()\n",
    "\n",
    "# Print the loss\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CLIPModel\n",
    "clip_model = CLIPModel()\n",
    "\n",
    "# Assume we have an anchor, positive, and negative ECG signals from the PhysioNetDataset\n",
    "_, anchor_ecg_signal = train_set[0]  # Get the first sample from the training set\n",
    "_, positive_ecg_signal = train_set[1]  # Get the second sample from the training set\n",
    "_, negative_ecg_signal = train_set[2]  # Get the third sample from the training set\n",
    "\n",
    "# We'll just use one of the leads for this example\n",
    "lead_name = list(anchor_ecg_signal.keys())[0]  # Get the name of the first lead\n",
    "anchor_lead_ecg_signal = anchor_ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "positive_lead_ecg_signal = positive_ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "negative_lead_ecg_signal = negative_ecg_signal[lead_name]  # Get the ECG signal for that lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\navme\\Desktop\\ECG_Project\\Notebooks\\ClinicalBERT.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Compute the triplet loss for these ECG signals\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m triplet_loss \u001b[39m=\u001b[39m clip_model(anchor_lead_ecg_signal, positive_lead_ecg_signal, negative_lead_ecg_signal)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Now triplet_loss is a tensor containing the triplet loss for these ECG signals\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(triplet_loss)\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\navme\\Desktop\\ECG_Project\\Notebooks\\ClinicalBERT.ipynb Cell 18\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, anchor, positive, negative):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     anchor_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mecg_encoder(anchor)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     positive_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mecg_encoder(positive)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     negative_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mecg_encoder(negative)\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\navme\\Desktop\\ECG_Project\\Notebooks\\ClinicalBERT.ipynb Cell 18\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     metadata, ecg_signal \u001b[39m=\u001b[39m data\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mfor\u001b[39;00m lead_info, lead_signal \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(metadata[\u001b[39m'\u001b[39m\u001b[39mleads_info\u001b[39m\u001b[39m'\u001b[39m], ecg_signal):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m# Create a textual description of the ECG signal\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Compute the triplet loss for these ECG signals\n",
    "triplet_loss = clip_model(anchor_lead_ecg_signal, positive_lead_ecg_signal, negative_lead_ecg_signal)\n",
    "\n",
    "# Now triplet_loss is a tensor containing the triplet loss for these ECG signals\n",
    "print(triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CLIPModel\n",
    "clip_model = CLIPModel()\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(clip_model.parameters())\n",
    "\n",
    "# Loop over your training data\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(train_set)):\n",
    "        # Get the anchor, positive, and negative ECG signals\n",
    "        _, anchor_ecg_signal = train_set[i]\n",
    "        _, positive_ecg_signal = train_set[(i + 1) % len(train_set)]  # Use the next sample as the positive example\n",
    "        _, negative_ecg_signal = train_set[(i + 2) % len(train_set)]  # Use the sample after that as the negative example\n",
    "\n",
    "        # We'll just use one of the leads for this example\n",
    "        lead_name = list(anchor_ecg_signal.keys())[0]  # Get the name of the first lead\n",
    "        anchor_lead_ecg_signal = anchor_ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "        positive_lead_ecg_signal = positive_ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "        negative_lead_ecg_signal = negative_ecg_signal[lead_name]  # Get the ECG signal for that lead\n",
    "\n",
    "        # Compute the triplet loss for these ECG signals\n",
    "        triplet_loss = clip_model(anchor_lead_ecg_signal, positive_lead_ecg_signal, negative_lead_ecg_signal)\n",
    "\n",
    "        # Backpropagate the loss and update the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {triplet_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\navme\\Desktop\\ECG_Project\\Notebooks\\ClinicalBERT.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X35sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X35sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     triplet_loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X35sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/navme/Desktop/ECG_Project/Notebooks/ClinicalBERT.ipynb#X35sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00mtriplet_loss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    389\u001b[0m     denom \u001b[39m=\u001b[39m (max_exp_avg_sqs[i]\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39;49msqrt() \u001b[39m/\u001b[39;49m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m    393\u001b[0m param\u001b[39m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39mstep_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate the CLIPModel\n",
    "clip_model = CLIPModel()\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = torch.optim.Adam(clip_model.parameters())\n",
    "\n",
    "# Loop over your training data\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(train_set)):\n",
    "        # Get the anchor, positive, and negative ECG signals\n",
    "        anchor_metadata, anchor_ecg_signal = train_set[i]\n",
    "        positive_metadata, positive_ecg_signal = train_set[(i + 1) % len(train_set)]  # Use the next sample as the positive example\n",
    "        negative_metadata, negative_ecg_signal = train_set[(i + 2) % len(train_set)]  # Use the sample after that as the negative example\n",
    "\n",
    "        # Compute the triplet loss for these ECG signals\n",
    "        triplet_loss = clip_model((anchor_metadata, anchor_ecg_signal), (positive_metadata, positive_ecg_signal), (negative_metadata, negative_ecg_signal))\n",
    "\n",
    "        # Backpropagate the loss and update the model parameters\n",
    "        optimizer.zero_grad()\n",
    "        triplet_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {triplet_loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The `ECGEncoder` is used to convert the raw ECG signals into embeddings using the ClinicalBERT model.\n",
    "\n",
    "2. The `CLIPModel` uses the `ECGEncoder` to get embeddings for the anchor, positive, and negative examples, and then computes the triplet loss on these embeddings. By training the `CLIPModel` on your dataset, you're effectively learning a new representation for your ECG signals where similar signals are close together and different signals are far apart.\n",
    "\n",
    "3. Once you've trained the `CLIPModel`, you can use it to transform your ECG signals into this new representation. You can then train a classifier on these transformed signals. This classifier could be any type of model you choose, such as a linear classifier, a support vector machine, a decision tree, a neural network, etc. The hope is that the new representation learned by the `CLIPModel` will be more useful for classification than the raw ECG signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the data\n",
    "train_dataset = PhysioNetDataset(processed_train_csv)\n",
    "val_dataset = PhysioNetDataset(processed_val_csv)\n",
    "\n",
    "# Instantiate the CLIPModel\n",
    "clip_model = CLIPModel()\n",
    "\n",
    "# Define the 1D CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(64, num_classes)  # num_classes is the number of unique values in 'dx_modality'\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "cnn = CNN()\n",
    "\n",
    "# Define a loss function and an optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn.parameters())\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(train_dataset)):\n",
    "        # Get the ECG signal and the label\n",
    "        _, ecg_signal = train_dataset[i]\n",
    "        label = processed_train_csv['dx_modality'][i]\n",
    "\n",
    "        # Transform the ECG signal using the CLIPModel\n",
    "        ecg_signal = clip_model(ecg_signal)\n",
    "\n",
    "        # Train the CNN on the transformed ECG signal and the label\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(ecg_signal)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {loss.item()}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(val_dataset)):\n",
    "        # Get the ECG signal and the label\n",
    "        _, ecg_signal = val_dataset[i]\n",
    "        label = processed_val_csv['dx_modality'][i]\n",
    "\n",
    "        # Transform the ECG signal using the CLIPModel\n",
    "        ecg_signal = clip_model(ecg_signal)\n",
    "\n",
    "        # Compute the model's prediction\n",
    "        outputs = cnn(ecg_signal)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "print(f\"Accuracy on the validation set: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict the class of a new ECG signal using the trained 1D CNN model, you can follow these steps:\n",
    "\n",
    "1. Load the new ECG signal.\n",
    "2. Transform the ECG signal into the learned representation using the `CLIPModel`.\n",
    "3. Pass the transformed ECG signal through the 1D CNN to get the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "# Load the new ECG signal\n",
    "new_ecg_signal = loadmat('new_ecg_signal.mat')\n",
    "\n",
    "# Assuming the ECG signal is stored in a variable named 'ecg' in the .mat file\n",
    "new_ecg_signal = new_ecg_signal['ecg']\n",
    "\n",
    "# Transform the ECG signal using the CLIPModel\n",
    "new_ecg_signal = clip_model(new_ecg_signal)\n",
    "\n",
    "# Pass the transformed ECG signal through the 1D CNN to get the predicted class\n",
    "outputs = cnn(new_ecg_signal)\n",
    "_, predicted_class = torch.max(outputs.data, 1)\n",
    "\n",
    "print(f\"The predicted class for the new ECG signal is: {predicted_class.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code loads a new ECG signal, transforms it using the `CLIPModel`, and then passes the transformed signal through the 1D CNN to get the predicted class. The predicted class is then printed to the console. Note that you'll need to replace `'new_ecg_signal.mat'` with the path to your new ECG signal file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neurokit2\n",
      "  Obtaining dependency information for neurokit2 from https://files.pythonhosted.org/packages/b4/22/e7e3b341b80a1e56f270a0137d4a3e6b20a58ae56c77785c0f6d3c6ba653/neurokit2-0.2.7-py2.py3-none-any.whl.metadata\n",
      "  Downloading neurokit2-0.2.7-py2.py3-none-any.whl.metadata (37 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from neurokit2) (1.24.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from neurokit2) (2.0.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from neurokit2) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from neurokit2) (1.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from neurokit2) (3.7.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn>=1.0.0->neurokit2) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn>=1.0.0->neurokit2) (3.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->neurokit2) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\navme\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->neurokit2) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->neurokit2) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\navme\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib->neurokit2) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->neurokit2) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->neurokit2) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->neurokit2) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->neurokit2) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from matplotlib->neurokit2) (6.0.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->neurokit2) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->neurokit2) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->neurokit2) (3.16.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\navme\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->neurokit2) (1.16.0)\n",
      "Downloading neurokit2-0.2.7-py2.py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.2/1.3 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.5/1.3 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.3/1.3 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 6.8 MB/s eta 0:00:00\n",
      "Installing collected packages: neurokit2\n",
      "Successfully installed neurokit2-0.2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install neurokit2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val': array([[408.24601882, 408.24601882, 408.24601882, ..., -83.34581329,\n",
       "         -74.965045  , -63.10339951],\n",
       "        [-92.07603073, -92.07603073, -92.07603073, ...,  57.20010276,\n",
       "          54.51591647,  58.88514819],\n",
       "        [225.08001192, 225.08001192, 225.08001192, ...,  93.39571052,\n",
       "          97.44912853, 117.96825132]])}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
