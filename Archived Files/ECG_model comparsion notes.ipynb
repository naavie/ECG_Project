{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Users\\navme\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyResidualBlock(nn.Module):\n",
    "    def __init__(self, downsample):\n",
    "        super(MyResidualBlock,self).__init__()\n",
    "        self.downsample = downsample\n",
    "        self.stride = 2 if self.downsample else 1\n",
    "        K = 9\n",
    "        P = (K-1)//2\n",
    "        self.conv1 = nn.Conv2d(in_channels=256,\n",
    "                               out_channels=256,\n",
    "                               kernel_size=(1,K),\n",
    "                               stride=(1,self.stride),\n",
    "                               padding=(0,P),\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=256,\n",
    "                               out_channels=256,\n",
    "                               kernel_size=(1,K),\n",
    "                               padding=(0,P),\n",
    "                               bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.idfunc_0 = nn.AvgPool2d(kernel_size=(1,2),stride=(1,2))\n",
    "            self.idfunc_1 = nn.Conv2d(in_channels=256,\n",
    "                                      out_channels=256,\n",
    "                                      kernel_size=(1,1),\n",
    "                                      bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "        if self.downsample:\n",
    "            identity = self.idfunc_0(identity)\n",
    "            identity = self.idfunc_1(identity)\n",
    "\n",
    "        x = x+identity\n",
    "        return x\n",
    "\n",
    "class NN(nn.Module): \n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels = 12,\n",
    "                              out_channels = 256,\n",
    "                              kernel_size = (1, 5),\n",
    "                              padding = (0, 2),\n",
    "                              stride = (1, 2),\n",
    "                              bias = False)\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(256)\n",
    "        self.rb_0 = MyResidualBlock(downsample=True)\n",
    "        self.rb_1 = MyResidualBlock(downsample=True)\n",
    "        self.rb_2 = MyResidualBlock(downsample=True)\n",
    "        self.rb_3 = MyResidualBlock(downsample=True)\n",
    "\n",
    "        self.rb_4 = MyResidualBlock(downsample=False)\n",
    "    \n",
    "        self.pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "        self.fc_1 = nn.Linear(256, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn(self.conv(x[:, :, None, :])))\n",
    "\n",
    "        x = self.rb_0(x)\n",
    "        x = self.rb_1(x)\n",
    "        x = self.rb_2(x)\n",
    "        x = self.rb_3(x)\n",
    "\n",
    "        x = F.dropout(x,p=0.5, training=self.training)\n",
    "\n",
    "        x = self.rb_4(x)      \n",
    "\n",
    "        x = x.squeeze(2)\n",
    "        x = self.pool(x).squeeze(2)\n",
    "\n",
    "        x = self.fc_1(x)\n",
    "        return x\n",
    "\n",
    "class NN_v2(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels = 12,\n",
    "                              out_channels = 256,\n",
    "                              kernel_size = (1, 5),\n",
    "                              padding = (0, 2),\n",
    "                              stride = (1, 2),\n",
    "                              bias = False)\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(256)\n",
    "        self.rb_0 = MyResidualBlock(downsample=True)\n",
    "        self.rb_0_add1 = MyResidualBlock(downsample=False)\n",
    "        self.rb_1 = MyResidualBlock(downsample=True)\n",
    "        self.rb_1_add1 = MyResidualBlock(downsample=False)\n",
    "        self.rb_2 = MyResidualBlock(downsample=True)\n",
    "        self.rb_2_add1 = MyResidualBlock(downsample=False)\n",
    "        self.rb_3 = MyResidualBlock(downsample=True)\n",
    "        self.rb_3_add1 = MyResidualBlock(downsample=False)\n",
    "\n",
    "        self.rb_4 = MyResidualBlock(downsample=False)\n",
    "        self.rb_4_add1 = MyResidualBlock(downsample=False)\n",
    "    \n",
    "        self.pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "        self.fc_1 = nn.Linear(256, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn(self.conv(x[:, :, None, :])))\n",
    "\n",
    "        x = self.rb_0(x)\n",
    "        x = self.rb_0_add1(x)\n",
    "        x = self.rb_1(x)\n",
    "        x = self.rb_1_add1(x)\n",
    "        x = self.rb_2(x)\n",
    "        x = self.rb_2_add1(x)\n",
    "        x = self.rb_3(x)\n",
    "        x = self.rb_3_add1(x)\n",
    "\n",
    "        x = F.dropout(x,p=0.5, training=self.training)\n",
    "\n",
    "        x = self.rb_4(x)   \n",
    "        x = self.rb_4_add1(x)\n",
    "\n",
    "        x = x.squeeze(2)\n",
    "        x = self.pool(x).squeeze(2)\n",
    "\n",
    "        x = self.fc_1(x)\n",
    "        return x\n",
    "\n",
    "class NN_v3(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels = 12,\n",
    "                              out_channels = 256,\n",
    "                              kernel_size = (1, 5),\n",
    "                              padding = (0, 2),\n",
    "                              stride = (1, 2),\n",
    "                              bias = False)\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(256)\n",
    "        self.rb_0 = MyResidualBlock(downsample=True)\n",
    "        self.rb_0_add1 = MyResidualBlock(downsample=False)\n",
    "        self.rb_0_add2 = MyResidualBlock(downsample=False)\n",
    "        self.rb_1 = MyResidualBlock(downsample=True)\n",
    "        self.rb_1_add1 = MyResidualBlock(downsample=False)\n",
    "        self.rb_1_add2 = MyResidualBlock(downsample=False)\n",
    "        self.rb_2 = MyResidualBlock(downsample=True)\n",
    "        self.rb_2_add1 = MyResidualBlock(downsample=False)\n",
    "        self.rb_2_add2 = MyResidualBlock(downsample=False)\n",
    "        self.rb_3 = MyResidualBlock(downsample=True)\n",
    "        self.rb_3_add1 = MyResidualBlock(downsample=False)\n",
    "        self.rb_3_add2 = MyResidualBlock(downsample=False)\n",
    "\n",
    "        self.rb_4 = MyResidualBlock(downsample=False)\n",
    "        self.rb_4_add1 = MyResidualBlock(downsample=False)\n",
    "        self.rb_4_add2 = MyResidualBlock(downsample=False)\n",
    "    \n",
    "        self.pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "        self.fc_1 = nn.Linear(256, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn(self.conv(x[:, :, None, :])))\n",
    "\n",
    "        x = self.rb_0(x)\n",
    "        x = self.rb_0_add1(x)\n",
    "        x = self.rb_0_add2(x)\n",
    "        x = self.rb_1(x)\n",
    "        x = self.rb_1_add1(x)\n",
    "        x = self.rb_1_add2(x)\n",
    "        x = self.rb_2(x)\n",
    "        x = self.rb_2_add1(x)\n",
    "        x = self.rb_2_add2(x)\n",
    "        x = self.rb_3(x)\n",
    "        x = self.rb_3_add1(x)\n",
    "        x = self.rb_3_add2(x)\n",
    "\n",
    "        x = F.dropout(x,p=0.5, training=self.training)\n",
    "\n",
    "        x = self.rb_4(x)   \n",
    "        x = self.rb_4_add1(x)\n",
    "        x = self.rb_4_add2(x)\n",
    "\n",
    "        x = x.squeeze(2)\n",
    "        x = self.pool(x).squeeze(2)\n",
    "\n",
    "        x = self.fc_1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(NN,self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels = 12,\n",
    "                              out_channels = 256,\n",
    "                              kernel_size = (1, 5),\n",
    "                              padding = (0, 2),\n",
    "                              stride = (1, 2),\n",
    "                              bias = False)\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(256)\n",
    "        self.rb_0 = MyResidualBlock(downsample=True)\n",
    "        self.rb_1 = MyResidualBlock(downsample=True)\n",
    "        self.rb_2 = MyResidualBlock(downsample=True)\n",
    "        self.rb_3 = MyResidualBlock(downsample=True)\n",
    "\n",
    "        self.rnn = torch.nn.GRU(256, 128, batch_first=True, bidirectional=True, num_layers=1)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "\n",
    "        self.fc_1 = nn.Linear(256, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn(self.conv(x[:, :, None, :])))\n",
    "\n",
    "        x = self.rb_0(x)\n",
    "        x = self.rb_1(x)\n",
    "        x = self.rb_2(x)\n",
    "        x = self.rb_3(x)\n",
    "\n",
    "        x = F.dropout(x,p=0.5, training=self.training)\n",
    "        x = x.squeeze(2)\n",
    "        x, s = self.rnn(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pool(x).squeeze(2)\n",
    "        x = self.fc_1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MyResidualBlock Class\n",
    "\n",
    "The MyResidualBlock class defines a residual block used in the neural network models. Here's a breakdown of its components and functionality:\n",
    "\n",
    "- **Initialization (__init__ method)**:\n",
    "  - **Parameters**:\n",
    "    - downsample: A boolean indicating whether to downsample the input.\n",
    "  - **Attributes**:\n",
    "    - stride: Set to 2 if downsampling, otherwise 1.\n",
    "    - conv1: First convolutional layer with a kernel size of 9, stride based on downsample, and padding to maintain the input size.\n",
    "    - bn1: Batch normalization layer for the output of conv1.\n",
    "    - conv2: Second convolutional layer with the same kernel size and padding.\n",
    "    - bn2: Batch normalization layer for the output of conv2.\n",
    "    - idfunc_0: Average pooling layer for downsampling the identity connection (used if downsample is True).\n",
    "    - idfunc_1: Convolutional layer for adjusting the number of channels in the identity connection (used if downsample is True).\n",
    "\n",
    "- **Forward Pass (forward method)**:\n",
    "  - **Input**: x, the input tensor.\n",
    "  - **Operations**:\n",
    "    - Store the input tensor in identity.\n",
    "    - Apply the first convolutional layer (conv1) followed by batch normalization (bn1) and Leaky ReLU activation.\n",
    "    - Apply the second convolutional layer (conv2) followed by batch normalization (bn2) and Leaky ReLU activation.\n",
    "    - If downsample is True:\n",
    "      - Downsample the identity tensor using idfunc_0.\n",
    "      - Adjust the number of channels in the identity tensor using idfunc_1.\n",
    "    - Add the processed input tensor (x) to the identity tensor.\n",
    "  - **Output**: The result of adding the processed input tensor to the identity tensor, which is the output of the residual block.\n",
    "\n",
    "### Summary\n",
    "- **Purpose**: The MyResidualBlock class implements a residual block with optional downsampling, which helps in training deep neural networks by allowing gradients to flow through the network more easily.\n",
    "- **Components**: Two convolutional layers with batch normalization and Leaky ReLU activation, and optional downsampling for the identity connection.\n",
    "- **Functionality**: Processes the input tensor through the convolutional layers and adds it to the identity tensor, enabling residual learning.\n",
    "\n",
    "\n",
    "Sure, here is a line-by-line breakdown of the `MyResidualBlock` class:\n",
    "\n",
    "### Initialization (`__init__` method)\n",
    "\n",
    "```python\n",
    "class MyResidualBlock(nn.Module):\n",
    "    def __init__(self, downsample):\n",
    "        super(MyResidualBlock, self).__init__()\n",
    "```\n",
    "- **Purpose**: Defines a class `MyResidualBlock` that inherits from `nn.Module`.\n",
    "- **Parameters**: downsample is a boolean indicating whether to downsample the input.\n",
    "- **Initialization**: Calls the parent class (`nn.Module`) initializer.\n",
    "\n",
    "```python\n",
    "        self.downsample = downsample\n",
    "        self.stride = 2 if self.downsample else 1\n",
    "```\n",
    "- **Purpose**: Sets the downsample attribute and determines the stride for the convolutional layers.\n",
    "- **Details**: If downsample is `True`, the stride is set to 2 (downsampling); otherwise, it is set to 1.\n",
    "\n",
    "```python\n",
    "        K = 9\n",
    "        P = (K-1)//2\n",
    "```\n",
    "- **Purpose**: Defines the kernel size (`K`) and padding (`P`) for the convolutional layers.\n",
    "- **Details**: Kernel size is 9, and padding is calculated to maintain the input size.\n",
    "\n",
    "```python\n",
    "        self.conv1 = nn.Conv2d(in_channels=256,\n",
    "                               out_channels=256,\n",
    "                               kernel_size=(1, K),\n",
    "                               stride=(1, self.stride),\n",
    "                               padding=(0, P),\n",
    "                               bias=False)\n",
    "```\n",
    "- **Purpose**: Defines the first convolutional layer.\n",
    "- **Details**: \n",
    "  - `in_channels` and `out_channels` are both 256.\n",
    "  - Kernel size is `(1, 9)`.\n",
    "  - Stride is `(1, self.stride)`.\n",
    "  - Padding is `(0, 4)`.\n",
    "  - Bias is set to `False`.\n",
    "\n",
    "```python\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "```\n",
    "- **Purpose**: Defines the first batch normalization layer.\n",
    "- **Details**: Normalizes the output of the first convolutional layer.\n",
    "\n",
    "```python\n",
    "        self.conv2 = nn.Conv2d(in_channels=256,\n",
    "                               out_channels=256,\n",
    "                               kernel_size=(1, K),\n",
    "                               padding=(0, P),\n",
    "                               bias=False)\n",
    "```\n",
    "- **Purpose**: Defines the second convolutional layer.\n",
    "- **Details**: \n",
    "  - `in_channels` and `out_channels` are both 256.\n",
    "  - Kernel size is `(1, 9)`.\n",
    "  - Padding is `(0, 4)`.\n",
    "  - Bias is set to `False`.\n",
    "\n",
    "```python\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "```\n",
    "- **Purpose**: Defines the second batch normalization layer.\n",
    "- **Details**: Normalizes the output of the second convolutional layer.\n",
    "\n",
    "```python\n",
    "        if self.downsample:\n",
    "            self.idfunc_0 = nn.AvgPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "            self.idfunc_1 = nn.Conv2d(in_channels=256,\n",
    "                                      out_channels=256,\n",
    "                                      kernel_size=(1, 1),\n",
    "                                      bias=False)\n",
    "```\n",
    "- **Purpose**: Defines the identity function layers for downsampling.\n",
    "- **Details**: \n",
    " - If downsample is `True`, an average pooling layer (`idfunc_0`) and a convolutional layer (`idfunc_1`) are defined.\n",
    "  - `idfunc_0` reduces the spatial dimensions by half.\n",
    "  - `idfunc_1` adjusts the number of channels to match the output of the main path.\n",
    "\n",
    "### Forward Pass (`forward` method)\n",
    "\n",
    "```python\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "```\n",
    "- **Purpose**: Defines the forward pass of the residual block.\n",
    "- **Details**: Stores the input tensor `x` in `identity` for the skip connection.\n",
    "\n",
    "```python\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)))\n",
    "```\n",
    "- **Purpose**: Applies the first convolutional layer, followed by batch normalization and Leaky ReLU activation.\n",
    "- **Details**: \n",
    "  - `self.conv1(x)` applies the first convolution.\n",
    "  - `self.bn1(...)` normalizes the output.\n",
    "  - `F.leaky_relu(...)` applies the Leaky ReLU activation function.\n",
    "\n",
    "```python\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)))\n",
    "```\n",
    "- **Purpose**: Applies the second convolutional layer, followed by batch normalization and Leaky ReLU activation.\n",
    "- **Details**: \n",
    "  - `self.conv2(x)` applies the second convolution.\n",
    "  - `self.bn2(...)` normalizes the output.\n",
    "  - `F.leaky_relu(...)` applies the Leaky ReLU activation function.\n",
    "\n",
    "```python\n",
    "        if self.downsample:\n",
    "            identity = self.idfunc_0(identity)\n",
    "            identity = self.idfunc_1(identity)\n",
    "```\n",
    "- **Purpose**: Applies the identity function layers if downsampling is required.\n",
    "- **Details**: \n",
    "  - `self.idfunc_0(identity)` applies average pooling to downsample the identity tensor.\n",
    "  - `self.idfunc_1(identity)` applies a convolution to adjust the number of channels.\n",
    "\n",
    "```python\n",
    "        x = x + identity\n",
    "```\n",
    "- **Purpose**: Adds the processed input tensor `x` to the identity tensor.\n",
    "- **Details**: This is the core of the residual connection, enabling the network to learn residual functions.\n",
    "\n",
    "```python\n",
    "        return x\n",
    "```\n",
    "- **Purpose**: Returns the output tensor.\n",
    "- **Details**: The output tensor is the result of adding the processed input tensor to the identity tensor.\n",
    "\n",
    "### Summary\n",
    "- **MyResidualBlock**: Implements a residual block with optional downsampling.\n",
    "- **Components**: Two convolutional layers with batch normalization and Leaky ReLU activation, and optional downsampling for the identity connection.\n",
    "- **Functionality**: Processes the input tensor through the convolutional layers and adds it to the identity tensor, enabling residual learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def visualize_residual_block(downsample):\n",
    "    dot = Digraph(comment='MyResidualBlock')\n",
    "\n",
    "    # Input node\n",
    "    dot.node('x', 'Input')\n",
    "\n",
    "    # Convolutional layers\n",
    "    dot.node('conv1', 'Conv2d\\n(1, 9)')\n",
    "    dot.node('bn1', 'BatchNorm2d')\n",
    "    dot.node('relu1', 'LeakyReLU')\n",
    "    dot.node('conv2', 'Conv2d\\n(1, 9)')\n",
    "    dot.node('bn2', 'BatchNorm2d')\n",
    "    dot.node('relu2', 'LeakyReLU')\n",
    "\n",
    "    # Identity path\n",
    "    if downsample:\n",
    "        dot.node('idfunc_0', 'AvgPool2d\\n(1, 2)')\n",
    "        dot.node('idfunc_1', 'Conv2d\\n(1, 1)')\n",
    "\n",
    "    # Output node\n",
    "    dot.node('output', 'Output')\n",
    "\n",
    "    # Edges for main path\n",
    "    dot.edges([('x', 'conv1'), ('conv1', 'bn1'), ('bn1', 'relu1'), ('relu1', 'conv2'), ('conv2', 'bn2'), ('bn2', 'relu2')])\n",
    "\n",
    "    # Edges for identity path\n",
    "    if downsample:\n",
    "        dot.edge('x', 'idfunc_0')\n",
    "        dot.edge('idfunc_0', 'idfunc_1')\n",
    "        dot.edge('idfunc_1', 'output')\n",
    "    else:\n",
    "        dot.edge('x', 'output')\n",
    "\n",
    "    # Merge paths\n",
    "    dot.edge('relu2', 'output')\n",
    "\n",
    "    # Render the graph\n",
    "    dot.render('residual_block', format='png', view=True)\n",
    "\n",
    "# Visualize the block with downsampling\n",
    "visualize_residual_block(downsample=True)\n",
    "\n",
    "# Visualize the block without downsampling\n",
    "visualize_residual_block(downsample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the main differences between the \n",
    "\n",
    "NN, NN_v2, and NN_v3 models:\n",
    "\n",
    "### NN Model\n",
    "- **Residual Blocks**: 5 blocks (4 downsampled, 1 non-downsampled)\n",
    "- **Structure**:\n",
    "  - rb_0 (downsampled)\n",
    "  - rb_1 (downsampled)\n",
    "  - rb_2 (downsampled)\n",
    "  - rb_3 (downsampled)\n",
    "  - rb_4 (non-downsampled)\n",
    "\n",
    "### NN_v2 Model\n",
    "- **Residual Blocks**: 9 blocks (4 downsampled, 5 non-downsampled)\n",
    "- **Structure**:\n",
    "  - rb_0 (downsampled)\n",
    "  - rb_0_add1 (non-downsampled)\n",
    "  - rb_1 (downsampled)\n",
    "  - rb_1_add1 (non-downsampled)\n",
    "  - rb_2 (downsampled)\n",
    "  - rb_2_add1 (non-downsampled)\n",
    "  - rb_3 (downsampled)\n",
    "  - rb_3_add1 (non-downsampled)\n",
    "  - rb_4 (non-downsampled)\n",
    "  - rb_4_add1 (non-downsampled)\n",
    "\n",
    "### NN_v3 Model\n",
    "- **Residual Blocks**: 14 blocks (4 downsampled, 10 non-downsampled)\n",
    "- **Structure**:\n",
    "  - rb_0 (downsampled)\n",
    "  - rb_0_add1 (non-downsampled)\n",
    "  - rb_0_add2 (non-downsampled)\n",
    "  - rb_1 (downsampled)\n",
    "  - rb_1_add1 (non-downsampled)\n",
    "  - rb_1_add2 (non-downsampled)\n",
    "  - rb_2 (downsampled)\n",
    "  - rb_2_add1 (non-downsampled)\n",
    "  - rb_2_add2 (non-downsampled)\n",
    "  - rb_3 (downsampled)\n",
    "  - rb_3_add1 (non-downsampled)\n",
    "  - rb_3_add2 (non-downsampled)\n",
    "  - rb_4 (non-downsampled)\n",
    "  - rb_4_add1 (non-downsampled)\n",
    "  - rb_4_add2 (non-downsampled)\n",
    "\n",
    "### Summary\n",
    "- **NN**: Simplest model with fewer residual blocks.\n",
    "- **NN_v2**: Adds additional non-downsampled residual blocks after each downsampled block.\n",
    "- **NN_v3**: Further increases the number of non-downsampled residual blocks, adding two after each downsampled block.\n",
    "\n",
    "These differences in the number and arrangement of residual blocks can affect the model's capacity and performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
